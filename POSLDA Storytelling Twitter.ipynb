{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Storytelling Twitter with POSLDA\n",
    "\n",
    "**Author: Yasir Abdurrahman**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background Information\n",
    "**Twitter** merupakan salah satu sosial media yang populer digunakan masyarakat Indonesia. Terbatasnya jumlah karakter *tweet* sebanyak 144 karakter, tidak menjadikan nilai informasi sebuah *tweet* berkurang. Hal ini dibuktikan dengan masuknya *trending topic* tagar **#PilkadaDKI** saat pemilihan gubernur bulan April 2017 kemarin seperti pada berita [CNN](https://www.cnnindonesia.com/teknologi/20170419143713-192-208647/tagar-quick-count-dan-pilkada-dki-kuasai-twitter/). Menilik ke belakang tahun 2014, saat Pemilihan Umum Legislatif (Pemilu) taggar **#IndonesiaElectionDay** juga masuk sebagai *trending topic* pada berita [Liputan6](http://tekno.liputan6.com/read/2034443/hashtag-bertema-pemilu-dominasi-trending-topic-twitter). \n",
    "\n",
    "Sebuah informasi sangatlah bernilai bagi jurnalis dalam membuat sebuah berita. Pengumpulan informasi dari Twitter masih terbatas berdasarkan *trending topic*, padahal masih banyak informasi yang dapat dimanfaatkan. Pengumpulan *tweet* berdasarkan radius lokasi, kemudian dikelompokkan berdasarkan topik tertentu, dan selanjutnya disusun menjadi sebuah *storytelling* dari kumpulan *tweet* yang memiliki topik yang sama akan menjadi bahan baru bagi para jurnalis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions for Investigation\n",
    "1. Bagaimana kesesuaian hasil dalam bentuk *storytelling* pada topik yang sama?\n",
    "2. Topik apakah yang paling sering dibicarakan di Twitter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "Dataset yang digunakan berupa 1000 *tweets* hasil *crawling* sendiri. Berikut adalah struktur dataset yang digunakan:\n",
    "```\n",
    "id_user      : id user\n",
    "username     : username pengguna twitter\n",
    "created_at   : tanggal dan waktu dibuatnya tweet\n",
    "latitude     : latitude\n",
    "longitude    : longitude\n",
    "text         : tweet\n",
    "```\n",
    "Dataset ditempatkan pada file dengan format *.csv*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Crawling\n",
    "#### Proses crawling\n",
    "1. Menggunakan library dari python yaitu twitter, dilakukan crawling berdasarkan radius dari koordinat latitude dan longtitude\n",
    "2. Data crawling meliputi atribut <b>id_user, username, created_at, latitude, longitude, text</b>\n",
    "3. Hasil crawling disimpan pada file bertipe .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import time\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "latitude = -7.059035     # geographical centre of search\n",
    "longitude = 110.443972   # geographical centre of search\n",
    "max_range = 10            # search range in kilometres\n",
    "outfile = \"tweets14.csv\"\n",
    "\n",
    "consumer_key = 'w6LdCwZHkdrIaNgxvuhZziSYe'\n",
    "consumer_secret = 'Y4guQcTeCwpXJ8UhYyYzCbazxTgHZqYLpN0maNJThXXCEfrMwz'\n",
    "access_token = '477563933-CIawAP0XgC8tjjXcRYmKaf4p0w2OqFAG4duYCqsL'\n",
    "access_secret = 'iZIjf2p4QaNyRE7wcibb1vNpIK2JntSRfrruoYRBMs6AS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csvfile = open(outfile, \"w\", newline='', encoding='utf-8')\n",
    "csvwriter = csv.writer(csvfile)\n",
    "\n",
    "row = [ \"id_user\", \"username\", \"created_at\", \"latitude\", \"longitude\", \"text\" ]\n",
    "csvwriter.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_count = 0\n",
    "start_time = time.clock()\n",
    "\n",
    "while result_count < 10000:\n",
    "    c = tweepy.Cursor(api.search,\n",
    "                      q=\"*\",\n",
    "                      count=10000,\n",
    "                      geocode = \"%f,%f,%dkm\" % (latitude, longitude, max_range)).items(10000)\n",
    "    while True:\n",
    "        try:\n",
    "            tweet = c.next()\n",
    "            if tweet.geo:\n",
    "                csvwriter.writerow([tweet.user.id, tweet.user.screen_name, tweet.created_at, \n",
    "                                    tweet.geo['coordinates'][0], tweet.geo['coordinates'][1], tweet.text])\n",
    "                result_count += 1\n",
    "                print (\"got %d results\" % result_count)\n",
    "            else:\n",
    "                csvwriter.writerow([tweet.user.id, tweet.user.screen_name, tweet.created_at, \n",
    "                                    None, None, tweet.text])\n",
    "                result_count += 1\n",
    "                print (\"got %d results\" % result_count)\n",
    "\n",
    "        except tweepy.TweepError:\n",
    "            print(\"sleeping\")\n",
    "            time.sleep(15 * 60)\n",
    "            continue\n",
    "        except StopIteration:\n",
    "            break\n",
    "csvfile.close()\n",
    "print(time.clock() - start_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "Hal yang dilakukan:\n",
    "1. Common Preprocessing\n",
    "    1. Removing symbols, ASCII strings, punctuations.\n",
    "    2. Tokenization\n",
    "    3. Case folding, convert into lowercase\n",
    "    4. Repeated dot (sedih... -> sedih.)\n",
    "    5. Repeated character ('hehe :)))' -> 'hehe :)')\n",
    "    6. Remove elipsis (lanjut baca... -> lanjut baca)\n",
    "    7. Repeated word that has meaning ('malam malam' -> 'malam-malam')\n",
    "    8. Remove newline\n",
    "2. Specific Preprocessing\n",
    "    1. Special symbols on Twitter, removing hashtag and mention\n",
    "    2. Remove all emoticons\n",
    "    3. Remove URL\n",
    "    4. Spell checker using Jaro Winkler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 6)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets = pd.read_csv('tweets12.csv')\n",
    "df_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_user</th>\n",
       "      <th>username</th>\n",
       "      <th>created_at</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>text</th>\n",
       "      <th>normalize</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2313451298</td>\n",
       "      <td>dikyock</td>\n",
       "      <td>2018-01-24 04:49:23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@rossonerifreak Masih kurang \\nButuh winger ta...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>245873655</td>\n",
       "      <td>phee_nophee</td>\n",
       "      <td>2018-01-24 04:49:21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apa yang Akan Berubah Untukmu Tahun Ini? https...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>955268887406264321</td>\n",
       "      <td>FahrulAditya16</td>\n",
       "      <td>2018-01-24 04:49:21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @Pesona_STW: Support by (˛`̯´̯)-☞\\n@Copasaj...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>791650560961241088</td>\n",
       "      <td>isarndr88</td>\n",
       "      <td>2018-01-24 04:49:10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apakah IPK penting?</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2395660873</td>\n",
       "      <td>Lukman_Prayogoo</td>\n",
       "      <td>2018-01-24 04:49:09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://t.co/YLyXSx3EVh</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id_user         username           created_at  latitude  \\\n",
       "0          2313451298          dikyock  2018-01-24 04:49:23       NaN   \n",
       "1           245873655      phee_nophee  2018-01-24 04:49:21       NaN   \n",
       "2  955268887406264321   FahrulAditya16  2018-01-24 04:49:21       NaN   \n",
       "3  791650560961241088        isarndr88  2018-01-24 04:49:10       NaN   \n",
       "4          2395660873  Lukman_Prayogoo  2018-01-24 04:49:09       NaN   \n",
       "\n",
       "   longitude                                               text normalize  \n",
       "0        NaN  @rossonerifreak Masih kurang \\nButuh winger ta...      None  \n",
       "1        NaN  Apa yang Akan Berubah Untukmu Tahun Ini? https...      None  \n",
       "2        NaN  RT @Pesona_STW: Support by (˛`̯´̯)-☞\\n@Copasaj...      None  \n",
       "3        NaN                                Apakah IPK penting?      None  \n",
       "4        NaN                            https://t.co/YLyXSx3EVh      None  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from modulenorm.normalize import Normalize\n",
    "from modulenorm.tokenize import Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx = 0\n",
    "df_tweets['normalize'] = None\n",
    "result = []\n",
    "for row in df_tweets['text']:\n",
    "    # normalize\n",
    "    norm = Normalize()\n",
    "    text_norm = norm.remove_ascii(row)\n",
    "    text_norm = norm.lower_text(text_norm)\n",
    "    text_norm = norm.repeat_char_modify(text_norm)\n",
    "    text_norm = norm.remove_elipsis(text_norm)\n",
    "    text_norm = norm.remove_newline(text_norm)\n",
    "    text_norm = norm.remove_url(text_norm)\n",
    "#     text_norm = norm.remove_emoticons(text_norm)\n",
    "    text_norm = norm.remove_hashtags_mentions(text_norm)\n",
    "    \n",
    "    tok = Tokenize()\n",
    "    text_norm = tok.WordTokenize(text_norm)\n",
    "#     text_norm = ' '.join(text_norm)\n",
    "    result.append(text_norm)\n",
    "    \n",
    "    text_norm = ' '.join(text_norm)\n",
    "#     df_tweets['normalize'][idx] = text_norm\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['gala', 'kehormatan', 'yg', 'diberikan', 'utk', 'org', 'yg', 'berjasa', 'utk', 'ranahminang', 'ini'], ['mudahkan', 'jangan', 'dipersulit'], ['rt', 'katakan', 'ssungguhnya', 'tdk', 'ada', 'seorg', 'pun', 'yg', 'dpt', 'melindungiku', 'dr', 'azab', 'allah', 'amp', 'aku', 'tdk', 'akan', 'mmperoleh', 'tempat', 'berlindung', 'selai'], ['rt', 'smg', 'pembahasan', 'rancangan', 'business', 'plan', 'pengem', 'industri', 'perik', 'tangkap', 'bbpi', 'semarang'], ['bismillahirrahmaanirrahiim', 'kirim', '25', 'dus', 'semoga', 'tambah', 'josssss']]\n"
     ]
    }
   ],
   "source": [
    "result2 = []\n",
    "i = 5\n",
    "for k in range(i):\n",
    "    result2.append(result[k+93])\n",
    "\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyjarowinkler import distance\n",
    "import operator\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fin = open(\"kamus.txt\", encoding='utf8')\n",
    "kamus = fin.readlines()\n",
    "kamus = [item.rstrip('\\n') for item in kamus]\n",
    "fin.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.62\n"
     ]
    }
   ],
   "source": [
    "result = distance.get_jaro_distance('yg', 'yang', winkler=True, scaling=0.1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet 1\n",
      "gala nagih distance: 0.47\n",
      "gala males distance: 0.63\n",
      "gala malas distance: 0.78\n",
      "gala Angola distance: 0.81\n",
      "gala agal distance: 0.83\n",
      "gala digalah distance: 0.86\n",
      "gala gaca distance: 0.87\n",
      "gala gagal distance: 0.88\n",
      "gala gala distance: 1.0\n",
      "kehormatan nagih distance: 0.43\n",
      "kehormatan rame distance: 0.45\n",
      "kehormatan seger distance: 0.53\n",
      "kehormatan Aceh distance: 0.57\n",
      "kehormatan AlQuran distance: 0.61\n",
      "kehormatan abdominal distance: 0.65\n",
      "kehormatan abnormal distance: 0.67\n",
      "kehormatan abnormalitas distance: 0.7\n",
      "kehormatan aeronotika distance: 0.72\n",
      "kehormatan airmata distance: 0.74\n",
      "kehormatan akhiran distance: 0.77\n",
      "kehormatan berhantam distance: 0.81\n",
      "kehormatan bermata distance: 0.82\n",
      "kehormatan bermuatan distance: 0.83\n",
      "kehormatan hormat distance: 0.87\n",
      "kehormatan kecermatan distance: 0.89\n",
      "kehormatan keharuman distance: 0.9\n",
      "kehormatan kehematan distance: 0.93\n",
      "kehormatan kehormatan distance: 1.0\n",
      "yg nagih distance: 0.57\n",
      "yg agak distance: 0.58\n",
      "yg ayo distance: 0.61\n",
      "yg kg distance: 0.67\n",
      "yg Yogyakarta distance: 0.73\n",
      "yg yoga distance: 0.85\n",
      "diberikan nagih distance: 0.44\n",
      "diberikan jutek distance: 0.54\n",
      "diberikan Afrika distance: 0.62\n",
      "diberikan Amerika distance: 0.69\n",
      "diberikan abolisikan distance: 0.7\n",
      "diberikan ademkan distance: 0.76\n",
      "diberikan beberkan distance: 0.81\n",
      "diberikan berikan distance: 0.83\n",
      "diberikan berisikan distance: 0.84\n",
      "diberikan bidikan distance: 0.88\n",
      "diberikan dibarengi distance: 0.89\n",
      "diberikan dibebani distance: 0.9\n",
      "diberikan dibeberkan distance: 0.94\n",
      "diberikan dibelikan distance: 0.96\n",
      "diberikan diberikan distance: 1.0\n",
      "utk tongpes distance: 0.49\n",
      "utk jutek distance: 0.69\n",
      "utk atik distance: 0.72\n",
      "utk HUT distance: 0.78\n",
      "utk tuak distance: 0.81\n",
      "utk utik  distance: 0.89\n",
      "org nagih distance: 0.51\n",
      "org tongpes distance: 0.65\n",
      "org argo distance: 0.72\n",
      "org bor distance: 0.78\n",
      "org borgol distance: 0.83\n",
      "org organ distance: 0.91\n",
      "org orgy distance: 0.94\n",
      "yg nagih distance: 0.57\n",
      "yg agak distance: 0.58\n",
      "yg ayo distance: 0.61\n",
      "yg kg distance: 0.67\n",
      "yg Yogyakarta distance: 0.73\n",
      "yg yoga distance: 0.85\n",
      "berjasa tongpes distance: 0.43\n",
      "berjasa males distance: 0.56\n",
      "berjasa Afganistan distance: 0.58\n",
      "berjasa Afrika distance: 0.59\n",
      "berjasa Albania distance: 0.62\n",
      "berjasa Amerika distance: 0.67\n",
      "berjasa aberasi distance: 0.86\n",
      "berjasa bejana distance: 0.88\n",
      "berjasa bekerjasama distance: 0.9\n",
      "berjasa beras distance: 0.93\n",
      "berjasa berasa distance: 0.97\n",
      "berjasa berjasa distance: 1.0\n",
      "utk tongpes distance: 0.49\n",
      "utk jutek distance: 0.69\n",
      "utk atik distance: 0.72\n",
      "utk HUT distance: 0.78\n",
      "utk tuak distance: 0.81\n",
      "utk utik  distance: 0.89\n",
      "ranahminang nagih distance: 0.51\n",
      "ranahminang rame distance: 0.65\n",
      "ranahminang Albania distance: 0.66\n",
      "ranahminang Argentina distance: 0.68\n",
      "ranahminang akunting distance: 0.69\n",
      "ranahminang alami distance: 0.72\n",
      "ranahminang anaemia distance: 0.8\n",
      "ranahminang ranah distance: 0.89\n",
      "ini nagih distance: 0.69\n",
      "ini anis distance: 0.72\n",
      "ini bini distance: 0.92\n",
      "ini inci distance: 0.93\n",
      "ini ini distance: 1.0\n",
      "Tweet 1  selesai dalam 195.68358882106804 seconds\n",
      "Tweet 2\n",
      "mudahkan nagih distance: 0.55\n",
      "mudahkan males distance: 0.6\n",
      "mudahkan mantab distance: 0.66\n",
      "mudahkan abaikan distance: 0.69\n",
      "mudahkan acuhkan distance: 0.81\n",
      "mudahkan adukan distance: 0.86\n",
      "mudahkan ampuhkan distance: 0.87\n",
      "mudahkan dirumahkan distance: 0.89\n",
      "mudahkan meludahkan distance: 0.94\n",
      "jangan nagih distance: 0.59\n",
      "jangan mantab distance: 0.67\n",
      "jangan Angola distance: 0.78\n",
      "jangan acungan distance: 0.85\n",
      "jangan angan distance: 0.94\n",
      "jangan jangan distance: 1.0\n",
      "dipersulit nagih distance: 0.43\n",
      "dipersulit tongpes distance: 0.58\n",
      "dipersulit Aries distance: 0.63\n",
      "dipersulit aberasi distance: 0.66\n",
      "dipersulit adhesi distance: 0.69\n",
      "dipersulit adpertensi distance: 0.75\n",
      "dipersulit berduit distance: 0.76\n",
      "dipersulit berkulit distance: 0.78\n",
      "dipersulit bersuit distance: 0.82\n",
      "dipersulit dibelit distance: 0.86\n",
      "dipersulit diparut distance: 0.87\n",
      "dipersulit dipelitur distance: 0.94\n",
      "dipersulit dipersulit distance: 1.0\n",
      "Tweet 2  selesai dalam 83.18170352399102 seconds\n",
      "Tweet 3\n",
      "rt tongpes distance: 0.55\n",
      "rt jutek distance: 0.57\n",
      "rt rame distance: 0.62\n",
      "rt Artik distance: 0.8\n",
      "rt arti distance: 0.83\n",
      "rt RT distance: 1.0\n",
      "katakan nagih distance: 0.45\n",
      "katakan jutek distance: 0.56\n",
      "katakan mantab distance: 0.64\n",
      "katakan Antarktik distance: 0.67\n",
      "katakan Antartik distance: 0.69\n",
      "katakan Antartika distance: 0.76\n",
      "katakan abaikan distance: 0.81\n",
      "katakan agakkan distance: 0.85\n",
      "katakan antarkan distance: 0.87\n",
      "katakan dikatakan distance: 0.93\n",
      "katakan katak distance: 0.94\n",
      "katakan katakan distance: 1.0\n",
      "ssungguhnya nagih distance: 0.43\n",
      "ssungguhnya tongpes distance: 0.49\n",
      "ssungguhnya seger distance: 0.57\n",
      "ssungguhnya abstinency distance: 0.59\n",
      "ssungguhnya acung distance: 0.62\n",
      "ssungguhnya acungan distance: 0.65\n",
      "ssungguhnya anggun distance: 0.7\n",
      "ssungguhnya asung distance: 0.72\n",
      "ssungguhnya bunganya distance: 0.77\n",
      "ssungguhnya kesungguhan distance: 0.84\n",
      "ssungguhnya sesungguhnya distance: 0.98\n",
      "tdk tongpes distance: 0.54\n",
      "tdk ada distance: 0.56\n",
      "tdk adakah distance: 0.67\n",
      "tdk adik distance: 0.72\n",
      "tdk dek distance: 0.78\n",
      "tdk tak distance: 0.8\n",
      "ada nagih distance: 0.51\n",
      "ada rame distance: 0.53\n",
      "ada malas distance: 0.69\n",
      "ada asatu distance: 0.72\n",
      "ada Andalas distance: 0.81\n",
      "ada ada distance: 1.0\n",
      "seorg nagih distance: 0.47\n",
      "seorg tongpes distance: 0.56\n",
      "seorg seger distance: 0.83\n",
      "seorg se distance: 0.84\n",
      "seorg seekor distance: 0.86\n",
      "seorg seorang distance: 0.94\n",
      "pun tongpes distance: 0.49\n",
      "pun jutek distance: 0.51\n",
      "pun aben distance: 0.53\n",
      "pun abu distance: 0.56\n",
      "pun acung distance: 0.69\n",
      "pun adun distance: 0.72\n",
      "pun apung distance: 0.87\n",
      "pun PN distance: 0.89\n",
      "pun paun distance: 0.92\n",
      "pun pun distance: 1.0\n",
      "yg nagih distance: 0.57\n",
      "yg agak distance: 0.58\n",
      "yg ayo distance: 0.61\n",
      "yg kg distance: 0.67\n",
      "yg Yogyakarta distance: 0.73\n",
      "yg yoga distance: 0.85\n",
      "dpt jutek distance: 0.51\n",
      "dpt ada distance: 0.56\n",
      "dpt adat distance: 0.72\n",
      "dpt Deptanhut distance: 0.78\n",
      "dpt Deptrans distance: 0.79\n",
      "dpt dep distance: 0.8\n",
      "dpt dipteri distance: 0.83\n",
      "dpt depth distance: 0.88\n",
      "melindungiku nagih distance: 0.43\n",
      "melindungiku males distance: 0.55\n",
      "melindungiku rame distance: 0.56\n",
      "melindungiku malas distance: 0.57\n",
      "melindungiku adendum distance: 0.63\n",
      "melindungiku afdeling distance: 0.64\n",
      "melindungiku aluminium distance: 0.69\n",
      "melindungiku bandung distance: 0.71\n",
      "melindungiku belandang distance: 0.72\n",
      "melindungiku beling distance: 0.75\n",
      "melindungiku bendung distance: 0.79\n",
      "melindungiku berlindung distance: 0.82\n",
      "melindungiku indung distance: 0.83\n",
      "melindungiku lindung distance: 0.86\n",
      "melindungiku melendung distance: 0.9\n",
      "melindungiku melindung distance: 0.95\n",
      "melindungiku melindungi distance: 0.97\n",
      "dr rame distance: 0.58\n",
      "dr ada distance: 0.61\n",
      "dr adrenalin distance: 0.74\n",
      "dr Dirjen distance: 0.78\n",
      "dr dara distance: 0.85\n",
      "dr dor distance: 0.9\n",
      "dr dry distance: 0.91\n",
      "azab zzzz distance: 0.5\n",
      "azab mantab distance: 0.75\n",
      "azab adab distance: 0.85\n",
      "azab azab distance: 1.0\n",
      "allah nagih distance: 0.6\n",
      "allah malas distance: 0.73\n",
      "allah Allah distance: 1.0\n",
      "amp nagih distance: 0.51\n",
      "amp males distance: 0.52\n",
      "amp rame distance: 0.72\n",
      "amp acap distance: 0.75\n",
      "amp amal distance: 0.78\n",
      "amp ampai distance: 0.91\n",
      "amp ampu distance: 0.94\n",
      "amp amp distance: 1.0\n",
      "aku nagih distance: 0.51\n",
      "aku rame distance: 0.53\n",
      "aku asatu distance: 0.56\n",
      "aku Agustus distance: 0.65\n",
      "aku Akpol distance: 0.69\n",
      "aku Akuarius distance: 0.79\n",
      "aku abu distance: 0.8\n",
      "aku aki distance: 0.82\n",
      "aku akrual distance: 0.87\n",
      "aku aku distance: 1.0\n",
      "tdk tongpes distance: 0.54\n",
      "tdk ada distance: 0.56\n",
      "tdk adakah distance: 0.67\n",
      "tdk adik distance: 0.72\n",
      "tdk dek distance: 0.78\n",
      "tdk tak distance: 0.8\n",
      "akan nagih distance: 0.48\n",
      "akan rame distance: 0.5\n",
      "akan mantab distance: 0.64\n",
      "akan asatu distance: 0.67\n",
      "akan Afganistan distance: 0.68\n",
      "akan Akuarius distance: 0.71\n",
      "akan Albania distance: 0.73\n",
      "akan Aswain distance: 0.75\n",
      "akan abang distance: 0.8\n",
      "akan adakan distance: 0.82\n",
      "akan adukan distance: 0.9\n",
      "akan akan distance: 1.0\n",
      "mmperoleh tongpes distance: 0.5\n",
      "mmperoleh males distance: 0.59\n",
      "mmperoleh Akpol distance: 0.64\n",
      "mmperoleh aerosol distance: 0.67\n",
      "mmperoleh ambrol distance: 0.7\n",
      "mmperoleh amper distance: 0.75\n",
      "mmperoleh ampere distance: 0.8\n",
      "mmperoleh beroleh distance: 0.84\n",
      "mmperoleh diperoleh distance: 0.85\n",
      "mmperoleh memperolah distance: 0.87\n",
      "mmperoleh peroleh distance: 0.93\n",
      "tempat tongpes distance: 0.59\n",
      "tempat Antarktik distance: 0.61\n",
      "tempat Antartik distance: 0.62\n",
      "tempat Athena distance: 0.67\n",
      "tempat alternatif distance: 0.69\n",
      "tempat amat distance: 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tempat berempat distance: 0.82\n",
      "tempat bertempat distance: 0.83\n",
      "tempat ditempa distance: 0.85\n",
      "tempat ditempat distance: 0.92\n",
      "tempat empat distance: 0.94\n",
      "tempat tempa distance: 0.97\n",
      "tempat tempat distance: 1.0\n",
      "berlindung nagih distance: 0.43\n",
      "berlindung seger distance: 0.53\n",
      "berlindung Akuarius distance: 0.56\n",
      "berlindung Amerika distance: 0.58\n",
      "berlindung Aprindo distance: 0.66\n",
      "berlindung aben distance: 0.68\n",
      "berlindung aubergine distance: 0.7\n",
      "berlindung Belanda distance: 0.74\n",
      "berlindung Belitung distance: 0.81\n",
      "berlindung bandering distance: 0.83\n",
      "berlindung bandung distance: 0.84\n",
      "berlindung belandang distance: 0.86\n",
      "berlindung beliung distance: 0.88\n",
      "berlindung bendung distance: 0.92\n",
      "berlindung berlinang distance: 0.94\n",
      "berlindung berlindung distance: 1.0\n",
      "selai nagih distance: 0.6\n",
      "selai males distance: 0.62\n",
      "selai seger distance: 0.68\n",
      "selai Aswain distance: 0.7\n",
      "selai alami distance: 0.73\n",
      "selai apelasi distance: 0.79\n",
      "selai belai distance: 0.87\n",
      "selai Selandia distance: 0.88\n",
      "selai sehelai distance: 0.92\n",
      "selai sela distance: 0.96\n",
      "selai selagi distance: 0.97\n",
      "selai selai distance: 1.0\n",
      "Tweet 3  selesai dalam 355.5196577785264 seconds\n",
      "Tweet 4\n",
      "rt tongpes distance: 0.55\n",
      "rt jutek distance: 0.57\n",
      "rt rame distance: 0.62\n",
      "rt Artik distance: 0.8\n",
      "rt arti distance: 0.83\n",
      "rt RT distance: 1.0\n",
      "smg nagih distance: 0.51\n",
      "smg rame distance: 0.53\n",
      "smg seger distance: 0.72\n",
      "smg sago distance: 0.75\n",
      "smg shg distance: 0.8\n",
      "smg smog distance: 0.93\n",
      "pembahasan nagih distance: 0.43\n",
      "pembahasan mantab distance: 0.61\n",
      "pembahasan abah distance: 0.68\n",
      "pembahasan abaikan distance: 0.69\n",
      "pembahasan ambah distance: 0.73\n",
      "pembahasan ambalan distance: 0.77\n",
      "pembahasan bahagian distance: 0.78\n",
      "pembahasan bahasa distance: 0.87\n",
      "pembahasan bahasan distance: 0.9\n",
      "pembahasan pemahaman distance: 0.93\n",
      "pembahasan pembacaan distance: 0.94\n",
      "pembahasan pembahas distance: 0.96\n",
      "pembahasan pembahasan distance: 1.0\n",
      "rancangan nagih distance: 0.37\n",
      "rancangan tongpes distance: 0.5\n",
      "rancangan rame distance: 0.66\n",
      "rancangan Afganistan distance: 0.7\n",
      "rancangan abangan distance: 0.73\n",
      "rancangan anaknya distance: 0.76\n",
      "rancangan ancaman distance: 0.79\n",
      "rancangan ancang distance: 0.89\n",
      "rancangan dirancangkan distance: 0.92\n",
      "rancangan pancangan distance: 0.93\n",
      "rancangan perancangan distance: 0.94\n",
      "rancangan rancang distance: 0.96\n",
      "rancangan rancangan distance: 1.0\n",
      "business nagih distance: 0.44\n",
      "business tongpes distance: 0.6\n",
      "business Aswain distance: 0.62\n",
      "business absen distance: 0.68\n",
      "business absensi distance: 0.81\n",
      "business bui distance: 0.83\n",
      "business bus distance: 0.85\n",
      "business busi distance: 0.9\n",
      "business buses distance: 0.91\n",
      "business busiest distance: 0.92\n",
      "business business distance: 1.0\n",
      "plan nagih distance: 0.48\n",
      "plan rame distance: 0.5\n",
      "plan mantab distance: 0.61\n",
      "plan Albania distance: 0.62\n",
      "plan Alpen distance: 0.85\n",
      "plan lan distance: 0.92\n",
      "plan pelan distance: 0.94\n",
      "plan plane distance: 0.96\n",
      "plan plan distance: 1.0\n",
      "pengem nagih distance: 0.58\n",
      "pengem tongpes distance: 0.64\n",
      "pengem seger distance: 0.7\n",
      "pengem bengek distance: 0.78\n",
      "pengem cengkeram distance: 0.8\n",
      "pengem mengadem distance: 0.82\n",
      "pengem pelinggam distance: 0.84\n",
      "pengem pen distance: 0.88\n",
      "pengem pendemo distance: 0.89\n",
      "pengem penemu distance: 0.92\n",
      "pengem pengebom distance: 0.95\n",
      "industri nagih distance: 0.44\n",
      "industri tongpes distance: 0.51\n",
      "industri mantab distance: 0.53\n",
      "industri Afganistan distance: 0.55\n",
      "industri Agustus distance: 0.6\n",
      "industri Antarktik distance: 0.65\n",
      "industri Antartik distance: 0.67\n",
      "industri Australi distance: 0.75\n",
      "industri agroindustri distance: 0.76\n",
      "industri baiduri distance: 0.78\n",
      "industri diantri distance: 0.81\n",
      "industri diindustrikan distance: 0.83\n",
      "industri induk distance: 0.86\n",
      "industri induksi distance: 0.92\n",
      "industri industri distance: 1.0\n",
      "perik nagih distance: 0.47\n",
      "perik jutek distance: 0.6\n",
      "perik Afrika distance: 0.7\n",
      "perik Amerika distance: 0.79\n",
      "perik beriak distance: 0.82\n",
      "perik epik distance: 0.85\n",
      "perik pegari distance: 0.86\n",
      "perik pelarik distance: 0.92\n",
      "perik percik distance: 0.96\n",
      "perik periuk distance: 0.97\n",
      "tangkap nagih distance: 0.57\n",
      "tangkap tongpes distance: 0.74\n",
      "tangkap Angola distance: 0.75\n",
      "tangkap ambungkan distance: 0.76\n",
      "tangkap aneka distance: 0.79\n",
      "tangkap anggap distance: 0.85\n",
      "tangkap angka distance: 0.9\n",
      "tangkap ditangkap distance: 0.93\n",
      "tangkap tangap distance: 0.97\n",
      "tangkap tangkap distance: 1.0\n",
      "bbpi nagih distance: 0.48\n",
      "bbpi Albania distance: 0.6\n",
      "bbpi April distance: 0.63\n",
      "bbpi abai distance: 0.67\n",
      "bbpi api distance: 0.72\n",
      "bbpi Babinsa distance: 0.73\n",
      "bbpi bab distance: 0.75\n",
      "bbpi babi distance: 0.85\n",
      "semarang nagih distance: 0.44\n",
      "semarang males distance: 0.55\n",
      "semarang rame distance: 0.6\n",
      "semarang mantab distance: 0.62\n",
      "semarang seger distance: 0.73\n",
      "semarang akseleran distance: 0.75\n",
      "semarang amtenar distance: 0.76\n",
      "semarang barang distance: 0.82\n",
      "semarang bersambang distance: 0.85\n",
      "semarang cemaran distance: 0.87\n",
      "semarang saran distance: 0.89\n",
      "semarang sarang distance: 0.92\n",
      "semarang sebarang distance: 0.93\n",
      "semarang semalang distance: 0.95\n",
      "semarang sembarang distance: 0.97\n",
      "Tweet 4  selesai dalam 220.53368817477167 seconds\n",
      "Tweet 5\n",
      "bismillahirrahmaanirrahiim nagih distance: 0.41\n",
      "bismillahirrahmaanirrahiim rame distance: 0.43\n",
      "bismillahirrahmaanirrahiim jos distance: 0.46\n",
      "bismillahirrahmaanirrahiim mantab distance: 0.47\n",
      "bismillahirrahmaanirrahiim Albania distance: 0.51\n",
      "bismillahirrahmaanirrahiim Australia distance: 0.52\n",
      "bismillahirrahmaanirrahiim abadiah distance: 0.58\n",
      "bismillahirrahmaanirrahiim abis distance: 0.62\n",
      "bismillahirrahmaanirrahiim antimalaria distance: 0.64\n",
      "bismillahirrahmaanirrahiim baiklah distance: 0.67\n",
      "bismillahirrahmaanirrahiim basmi distance: 0.69\n",
      "bismillahirrahmaanirrahiim beristirahatlah distance: 0.73\n",
      "bismillahirrahmaanirrahiim bis distance: 0.79\n",
      "bismillahirrahmaanirrahiim bismilah distance: 0.86\n",
      "bismillahirrahmaanirrahiim bismillah distance: 0.87\n",
      "kirim nagih distance: 0.47\n",
      "kirim rame distance: 0.63\n",
      "kirim Akuarius distance: 0.66\n",
      "kirim air distance: 0.69\n",
      "kirim akhir distance: 0.73\n",
      "kirim akhiri distance: 0.82\n",
      "kirim iri distance: 0.87\n",
      "kirim kir distance: 0.91\n",
      "kirim kiri distance: 0.96\n",
      "kirim kirim distance: 1.0\n",
      "dus jutek distance: 0.51\n",
      "dus jos distance: 0.56\n",
      "dus Agustus distance: 0.65\n",
      "dus adas distance: 0.72\n",
      "dus adu distance: 0.78\n",
      "dus diusahakan distance: 0.79\n",
      "dus diusap distance: 0.85\n",
      "dus dus distance: 1.0\n",
      "semoga nagih distance: 0.46\n",
      "semoga tongpes distance: 0.54\n",
      "semoga mantab distance: 0.56\n",
      "semoga seger distance: 0.76\n",
      "semoga cemong distance: 0.78\n",
      "semoga demograf distance: 0.82\n",
      "semoga moga distance: 0.89\n",
      "semoga sega distance: 0.91\n",
      "semoga sema distance: 0.92\n",
      "semoga semoga distance: 1.0\n",
      "tambah nagih distance: 0.58\n",
      "tambah rame distance: 0.61\n",
      "tambah mantab distance: 0.76\n",
      "tambah abah distance: 0.89\n",
      "tambah ambah distance: 0.94\n",
      "tambah tabah distance: 0.96\n",
      "tambah tambah distance: 1.0\n",
      "josssss tongpes distance: 0.52\n",
      "josssss jos distance: 0.87\n",
      "Tweet 5  selesai dalam 128.70501215906916 seconds\n",
      "983.6294417661866 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.clock()\n",
    "result2norm = []\n",
    "i = 1\n",
    "for tweet in result2:\n",
    "    tweet_time = time.clock()\n",
    "    temp_kalimat = []\n",
    "    print(\"Tweet\", i)\n",
    "    for token in tweet:\n",
    "        temp_kata = {'max': 0, 'kata': ''}\n",
    "        temp_kata['kata'] = token\n",
    "        for k in kamus:\n",
    "            result = distance.get_jaro_distance(token, k, winkler=True, scaling=0.1)\n",
    "            if result > temp_kata['max']:\n",
    "                temp_kata['max'] = result\n",
    "                temp_kata['kata'] = k\n",
    "                print(token, temp_kata['kata'], \"distance:\", result)\n",
    "        temp_kalimat.append(temp_kata['kata'])\n",
    "    result2norm.append(temp_kalimat)\n",
    "    print(\"Tweet\", i, \" selesai dalam\",time.clock() - tweet_time, \"seconds\")\n",
    "    i += 1\n",
    "print(time.clock() - start_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gala', 'kehormatan', 'yg', 'diberikan', 'utk', 'org', 'yg', 'berjasa', 'utk', 'ranahminang', 'ini']\n",
      "['mudahkan', 'jangan', 'dipersulit']\n",
      "['rt', 'katakan', 'ssungguhnya', 'tdk', 'ada', 'seorg', 'pun', 'yg', 'dpt', 'melindungiku', 'dr', 'azab', 'allah', 'amp', 'aku', 'tdk', 'akan', 'mmperoleh', 'tempat', 'berlindung', 'selai']\n",
      "['rt', 'smg', 'pembahasan', 'rancangan', 'business', 'plan', 'pengem', 'industri', 'perik', 'tangkap', 'bbpi', 'semarang']\n",
      "['bismillahirrahmaanirrahiim', 'kirim', '25', 'dus', 'semoga', 'tambah', 'josssss']\n",
      "\n",
      "['gala', 'kehormatan', 'yoga', 'diberikan', 'utik ', 'orgy', 'yoga', 'berjasa', 'utik ', 'ranah', 'ini']\n",
      "['meludahkan', 'jangan', 'dipersulit']\n",
      "['RT', 'katakan', 'sesungguhnya', 'tak', 'ada', 'seorang', 'pun', 'yoga', 'depth', 'melindungi', 'dry', 'azab', 'Allah', 'amp', 'aku', 'tak', 'akan', 'peroleh', 'tempat', 'berlindung', 'selai']\n",
      "['RT', 'smog', 'pembahasan', 'rancangan', 'business', 'plan', 'pengebom', 'industri', 'periuk', 'tangkap', 'babi', 'sembarang']\n",
      "['bismillah', 'kirim', '25', 'dus', 'semoga', 'tambah', 'jos']\n"
     ]
    }
   ],
   "source": [
    "for _ in result2:\n",
    "    print(_)\n",
    "print()\n",
    "for _ in result2norm:\n",
    "    print(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: 0\n",
      "Selesai dalam:  155.38980633868067 seconds\n",
      "Tweet: 1\n",
      "Selesai dalam:  201.95722402620544 seconds\n",
      "Tweet: 2\n",
      "Selesai dalam:  485.32538294651437 seconds\n",
      "Tweet: 3\n",
      "Selesai dalam:  656.2405779334999 seconds\n",
      "Tweet: 4\n",
      "Selesai dalam:  756.5421005966923 seconds\n",
      "756.543074462535 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "i = 0\n",
    "k = 0\n",
    "start_time = time.clock()\n",
    "lstkalimatbr = []\n",
    "lstdokumenbr = []\n",
    "for kalimat in result2:\n",
    "    print(\"Tweet:\", k)\n",
    "    lstkalimatbr = []\n",
    "    for kata in kalimat:\n",
    "        per_kata = {}\n",
    "        for kms in kamus:\n",
    "            result = distance.get_jaro_distance(repr(kata), repr(kms), winkler=True, scaling=0.1)\n",
    "            per_kata.update({i: (result, kata, kamus)})\n",
    "            i = i + 1\n",
    "        maxi = max(per_kata.items(), key=operator.itemgetter(1))\n",
    "        lstkalimatbr.append(maxi[1][2])\n",
    "    lstdokumenbr.append(lstkalimatbr)\n",
    "    print(\"Selesai dalam: \",time.clock() - start_time, \"seconds\")\n",
    "    k += 1\n",
    "print(time.clock() - start_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gala', 'kehormatan', 'yg', 'diberikan', 'utk', 'org', 'yg', 'berjasa', 'utk', 'ranahminang', 'ini']\n",
      "['mudahkan', 'jangan', 'dipersulit']\n",
      "['rt', 'katakan', 'ssungguhnya', 'tdk', 'ada', 'seorg', 'pun', 'yg', 'dpt', 'melindungiku', 'dr', 'azab', 'allah', 'amp', 'aku', 'tdk', 'akan', 'mmperoleh', 'tempat', 'berlindung', 'selai']\n",
      "['rt', 'smg', 'pembahasan', 'rancangan', 'business', 'plan', 'pengem', 'industri', 'perik', 'tangkap', 'bbpi', 'semarang']\n",
      "['bismillahirrahmaanirrahiim', 'kirim', '25', 'dus', 'semoga', 'tambah', 'josssss']\n"
     ]
    }
   ],
   "source": [
    "for _ in result2norm:\n",
    "    print(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fin = open(\"kamus.txt\", encoding='utf8')\n",
    "kamus = fin.readlines()\n",
    "kamus = [item.rstrip('\\n') for item in kamus]\n",
    "fin.close()\n",
    "\n",
    "i = 0\n",
    "lstkalimatbr = []\n",
    "lstdokumenbr = []\n",
    "for kalimat in result:\n",
    "    lstkalimatbr = []\n",
    "    for kata in kalimat:\n",
    "        per_kata = {}\n",
    "        for kms in kamus:\n",
    "            result = distance.get_jaro_distance(repr(kata), repr(kms), winkler=True, scaling=0.1)\n",
    "            per_kata.update({i: (result, kata, kamus)})\n",
    "            i = i + 1\n",
    "        maxi = max(per_kata.items(), key=operator.itemgetter(1))\n",
    "        lstkalimatbr.append(maxi[1][2])\n",
    "    lstdokumenbr.append(lstkalimatbr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in lstdokumenbr:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_tweets['normalize'].to_csv('nnn.csv', header=None, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx = 0\n",
    "result = []\n",
    "for row in df['text']:\n",
    "    \n",
    "    # normalize\n",
    "    usenorm = normalize()\n",
    "    text_norm = usenorm.remove_ascii(row) # normalisasi enter, 1 revw 1 baris\n",
    "    text_norm = usenorm.enterNormalize(text_norm) # normalisasi enter, 1 revw 1 baris\n",
    "    text_norm = usenorm.lowerNormalize(text_norm) # normalisasi huruf besar ke kecil\n",
    "    text_norm = usenorm.repeatcharNormalize(text_norm) # normalisasi titik yang berulang\n",
    "    text_norm = usenorm.linkNormalize(text_norm) # normalisasi link dalam text\n",
    "    text_norm = usenorm.spacecharNormalize(text_norm) # normalisasi spasi karakter\n",
    "    text_norm = usenorm.ellipsisNormalize(text_norm) # normalisasi elepsis (…)\n",
    "    \n",
    "    # tokenize\n",
    "    tok = tokenize() \n",
    "    text_norm = tok.WordTokenize(text_norm) # pisah tiap kata pada kalimat\n",
    "    result.append(text_norm)\n",
    "    \n",
    "    \n",
    "#     text_norm = usenorm.spellNormalize(text_norm) # cek spell dari kata perkata\n",
    "#     text_norm = usenorm.wordcNormalize(text_norm,2) # menyambung kata (malam-malam) (param: textlist, jmlh_loop)\n",
    "    # text_norm = usenorm.stemmingNormalize(text_norm,'word') # mengubah ke bentuk kata dasar (text, type_data)\n",
    "\n",
    "    text_norm = ' '.join(text_norm) # menggabung kalimat tokenize dengan separate spasi\n",
    "\n",
    "#     text_norm = usenorm.emoticonNormalize(text_norm) # menggabung struktur emoticon yang terpisah ([: - )] = [:-)])\n",
    "\n",
    "    # walking2\n",
    "    # konfer @ ke at untuk penunjuk tempat\n",
    "    \n",
    "#     output = open(\"output1.txt\",\"a\")\n",
    "#     output.write(str(text_norm))\n",
    "#     output.write('\\n')\n",
    "#     output.close()\n",
    "\n",
    "#     print(no, text_norm)\n",
    "#     df['text'][idx] = text_norm\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['text'].to_csv('ttt1.csv', header=['tweets'], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = df['text']\n",
    "texts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(df['text'][4])\n",
    "type(df['text'][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_ascii_replace(row):\n",
    "    return row.encode('ascii', 'replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = df['text']\n",
    "t = t.apply(encode_ascii_replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t[4].decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode(row):\n",
    "    return row.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "td = t.apply(decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "td.to_csv('text9.csv', header=['tweets'], index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts_df = pd.read_csv('text9.csv')\n",
    "texts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import html\n",
    "\n",
    "def html_parser(row):\n",
    "    return html.unescape(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tdf = texts_df['tweets']\n",
    "clean1 = tdf.apply(html_parser)\n",
    "clean1[22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decoding_data(row):\n",
    "    return row.encode('ascii', 'ignore').decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean2 = clean1.apply(decoding_data)\n",
    "clean2[53]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def standardizing_words(row):\n",
    "    return ''.join(''.join(s)[:2] for _, s in itertools.groupby(row) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean3 = clean2.apply(standardizing_words)\n",
    "print(clean2[49])\n",
    "clean3[49]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_url(row):\n",
    "    return re.sub(r\"http\\S+\", \"\", row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean4 = clean3.apply(clean_url)\n",
    "print(clean3[0])\n",
    "clean4[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_newline(row):\n",
    "    return ' '.join(row.splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean5 = clean4.apply(remove_newline)\n",
    "clean5[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from string import maketrans\n",
    "\n",
    "intab = \"=><;()^{}[]\\~\"\n",
    "outtab = \"\"\n",
    "trantab = maketrans(intab, outtab)\n",
    "\n",
    "def remove_unnecessary_punctuation(row):\n",
    "    return row.translate(trantab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create sample documents\n",
    "doc_a = \"Brokoli bagus untuk dimakan. Adikku suka makan brokoli, tetapi ibuku tidak.\"\n",
    "doc_b = \"Ibuku menghabiskan banyak waktu berkeliling melihat adikku latihan bisbol.\"\n",
    "doc_c = \"Beberapa ahli kesehatan menyarankan bahwa mengemudi dapat menyebabkan ketegangan dan tekanan darah meningkat.\"\n",
    "doc_d = \"Saya sering merasakan tekanan untuk tampil seperti saat presentasi di sekolah.\"\n",
    "doc_e = \"Profesional kesehatan mengatakan bahwa brokoli itu baik untuk kesehatan.\"\n",
    "doc_f = \"Teman saya seorang pemain bisbol yang pernah mendapatkan juara.\"\n",
    "doc_g = \"Pemain bisbol yang bernama Flash itu sangat suka memakan brokoli.\"\n",
    "doc_h = \"Sopir yang mengemudi taksi itu mendapatkan tekanan dari penumpangnya.\"\n",
    "doc_i = \"Saat bertanding, olahraga bisbol memberikan ketegangan dan meningkatkan tekanan darah para penonton.\"\n",
    "doc_j = \"Ibuku menyarankan saya untuk memakan brokoli agar tekanan darah terkontrol.\"\n",
    "\n",
    "# compile sample documents into a list\n",
    "docs = [doc_a, doc_b, doc_c, doc_d, doc_e, doc_f, doc_g, doc_h, doc_i, doc_j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "doc_j = \"Ibuku menyarankan saya untuk memakan brokoli agar tekanan darah terkontrol.\"\n",
    "a = [doc_j]\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+|\\$[\\d\\.]+')\n",
    "print(tokenizer.tokenize(doc_j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = []\n",
    "for doc in docs:\n",
    "    print(doc)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+|\\$[\\d\\.]+')\n",
    "    result.append(tokenizer.tokenize(doc))\n",
    "\n",
    "for i in result:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from modulenorm.tokenize import Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tok = Tokenize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
