{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class LanguageNgramModel:\n",
    "    \"\"\" \n",
    "    The model remembers and predicts which letters follow which.\n",
    "    Constructor parameters:\n",
    "        order - number of characters the model remembers, or n-1\n",
    "        smoothing - the number, added to each counter for stability\n",
    "        recursive - weight of the model of one order less\n",
    "    Learned parameters:\n",
    "        counter_ - storage of n-grams, as dict of counters  \n",
    "        vocabulary_ - set of characters that the model knows\n",
    "    \"\"\"\n",
    "    def __init__(self, order=1, smoothing=1.0, recursive=0.001):\n",
    "        self.order = order\n",
    "        self.smoothing = smoothing\n",
    "        self.recursive = recursive\n",
    "    \n",
    "    def fit(self, corpus):\n",
    "        \"\"\" Estimate freqency of all n-grams in the text\n",
    "        parameters:\n",
    "            corpus - a text string \n",
    "        \"\"\"\n",
    "        self.counter_ = defaultdict(lambda: Counter())\n",
    "        self.vocabulary_ = set()\n",
    "        for i, token in enumerate(corpus[self.order:]):\n",
    "            context = corpus[i:(i+self.order)]\n",
    "            self.counter_[context][token] += 1\n",
    "            self.vocabulary_.add(token)\n",
    "        self.vocabulary_ = sorted(list(self.vocabulary_))\n",
    "        if self.recursive > 0 and self.order > 0:\n",
    "            self.child_ = LanguageNgramModel(self.order-1, self.smoothing, self.recursive)\n",
    "            self.child_.fit(corpus)\n",
    "            \n",
    "    def get_counts(self, context):\n",
    "        \"\"\" Estimate frequency of all symbols that may follow the context\n",
    "        Parameters:\n",
    "            context - text string (only the last self.order chars matter)\n",
    "        Returns: \n",
    "            freq - vector of letter conditional frequencies, as pandas.Series\n",
    "        \"\"\"\n",
    "        if self.order:\n",
    "            local = context[-self.order:]\n",
    "        else:\n",
    "            local = ''\n",
    "        freq_dict = self.counter_[local]\n",
    "        freq = pd.Series(index=self.vocabulary_)\n",
    "        for i, token in enumerate(self.vocabulary_):\n",
    "            freq[token] = freq_dict[token] + self.smoothing\n",
    "        if self.recursive > 0 and self.order > 0:\n",
    "            child_freq = self.child_.get_counts(context) * self.recursive\n",
    "            freq += child_freq\n",
    "        return freq\n",
    "    \n",
    "    def predict_proba(self, context):\n",
    "        \"\"\" Estimate probability of all symbols that may follow the context\n",
    "        Parameters:\n",
    "            context - text string (only the last self.order chars matter)\n",
    "        Returns: \n",
    "            freq - vector of letter conditional frequencies, as pandas.Series\n",
    "        \"\"\"\n",
    "        counts = self.get_counts(context)\n",
    "        return counts / counts.sum()\n",
    "    \n",
    "    def single_log_proba(self, context, continuation):\n",
    "        \"\"\" Estimate log probability of the certain continuation of the context\n",
    "        Parameters:\n",
    "            context - text string, known beginning of the phrase\n",
    "            continuation - text string, its hypothetical end\n",
    "        Returns: \n",
    "            result - a float, log of probability\n",
    "        \"\"\"\n",
    "        result = 0.0\n",
    "        for token in continuation:\n",
    "            result += np.log(self.predict_proba(context)[token])\n",
    "            context += token\n",
    "        return result\n",
    "    \n",
    "    def single_proba(self, context, continuation):\n",
    "        \"\"\" Estimate probability of the certain continuation of the context\n",
    "        Parameters:\n",
    "            context - text string, known beginning of the phrase\n",
    "            continuation - text string, its hypothetical end\n",
    "        Returns: \n",
    "            result - a float, probability\n",
    "        \"\"\"\n",
    "        return np.exp(self.single_log_proba(context, continuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MissingLetterModel:\n",
    "    \"\"\" \n",
    "    The model remembers and predicts which letters are usually missed.\n",
    "    Constructor parameters:\n",
    "        order - number of characters the model remembers, or n-1\n",
    "        smoothing_missed - the number added to missed counter\n",
    "        smoothing_total - the number added to total counter\n",
    "    Learned parameters:\n",
    "        missed_counter_ - counter of occurences of the missed characters \n",
    "        total_counter_ - counter of occurences of all characters \n",
    "    \"\"\"\n",
    "    def __init__(self, order=0, smoothing_missed=0.3, smoothing_total=1.0):\n",
    "        self.order = order\n",
    "        self.smoothing_missed = smoothing_missed\n",
    "        self.smoothing_total = smoothing_total\n",
    "    \n",
    "    def fit(self, sentence_pairs):\n",
    "        \"\"\" Estimate of missing probability for each symbol\n",
    "        Parameters:\n",
    "            sentence_pairs - list of (original phrase, abbreviation)\n",
    "        In the abbreviation, all missed symbols are replaced with \"-\"\n",
    "        \"\"\"\n",
    "        self.missed_counter_ = defaultdict(lambda: Counter())\n",
    "        self.total_counter_ = defaultdict(lambda: Counter())\n",
    "        for (original, observed) in sentence_pairs:\n",
    "            for i, (original_letter, observed_letter) \\\n",
    "                    in enumerate(zip(original[self.order:], observed[self.order:])):\n",
    "                context = original[i:(i+self.order)]\n",
    "                if observed_letter == '-':\n",
    "                    self.missed_counter_[context][original_letter] += 1\n",
    "                self.total_counter_[context][original_letter] += 1 \n",
    "    \n",
    "    def predict_proba(self, context, last_letter):\n",
    "        \"\"\" Estimate of probability of last_letter being missed after context\"\"\"\n",
    "        if self.order:\n",
    "            local = context[-self.order:]\n",
    "        else:\n",
    "            local = ''\n",
    "        missed_freq = self.missed_counter_[local][last_letter] + self.smoothing_missed\n",
    "        total_freq = self.total_counter_[local][last_letter] + self.smoothing_total\n",
    "        return missed_freq / total_freq\n",
    "    \n",
    "    def single_log_proba(self, context, continuation, actual=None):\n",
    "        \"\"\" Estimate log probability that after context, \n",
    "            continuation is abbreviated to actual.\n",
    "        If actual is None, it is assumed that nothing is abbreviated.\n",
    "        \"\"\"\n",
    "        if not actual:\n",
    "            actual = continuation\n",
    "        result = 0.0\n",
    "        for orig_token, act_token in zip(continuation, actual):\n",
    "            pp = self.predict_proba(context, orig_token)\n",
    "            if act_token != '-':\n",
    "                pp = 1 - pp\n",
    "            result += np.log(pp)\n",
    "            context += orig_token\n",
    "        return result\n",
    "    \n",
    "    def single_proba(self, context, continuation, actual=None):\n",
    "        \"\"\" Estimate probability that after context, \n",
    "            continuation is abbreviated to actual.\n",
    "        If actual is None, it is assumed that nothing is abbreviated.\n",
    "        \"\"\"\n",
    "        return np.exp(self.single_log_proba(context, continuation, actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lang_model = LanguageNgramModel(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "missed_model = MissingLetterModel(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from heapq import heappush, heappop\n",
    "\n",
    "def generate_options(prefix_proba, prefix, suffix, \n",
    "                     lang_model, missed_model, optimism=0.5, cache=None):\n",
    "    \"\"\" Generate partial options of abbreviation decoding (a helper function)\n",
    "    Parameters:\n",
    "        prefix_proba - log probability of decoded part of the abbreviation\n",
    "        prefix - decoded part of the abbreviation\n",
    "        suffix - not decoded part of the abbreviation\n",
    "        lang_model - the language model\n",
    "        missed_model - the abbreviation probability model\n",
    "        optimism - coefficient for log likelihood of the word end\n",
    "        cache - storage of suffix likelihood estimates\n",
    "    Returns: list of options in the form (likelihood estimate, decoded part, \n",
    "        not decoded part, the new letter, the suffix likelihood estimate)\n",
    "    \"\"\"\n",
    "    options = []\n",
    "    for letter in lang_model.vocabulary_ + ['']:\n",
    "        if letter:  # here we assume the character was missing\n",
    "            next_letter = letter\n",
    "            new_suffix = suffix\n",
    "            new_prefix = prefix + next_letter\n",
    "            proba_missing_state = - np.log(missed_model.predict_proba(prefix, letter))\n",
    "        else:  # here we assume there was no missing character\n",
    "            next_letter = suffix[0]\n",
    "            new_suffix = suffix[1:]\n",
    "            new_prefix = prefix + next_letter\n",
    "            proba_missing_state = - np.log((1 - missed_model.predict_proba(prefix, next_letter)))\n",
    "        proba_next_letter = - np.log(lang_model.single_proba(prefix, next_letter))\n",
    "        if cache:\n",
    "            proba_suffix = cache[len(new_suffix)] * optimism\n",
    "        else:\n",
    "            proba_suffix = - np.log(lang_model.single_proba(new_prefix, new_suffix)) * optimism\n",
    "        proba = prefix_proba + proba_next_letter + proba_missing_state + proba_suffix\n",
    "        options.append((proba, new_prefix, new_suffix, letter, proba_suffix))\n",
    "    return options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def noisy_channel(word, lang_model, missed_model, freedom=3.0, \n",
    "                  max_attempts=10000, optimism=0.9, verbose=False):\n",
    "    \"\"\" Suggest phrases, for which word may be the abbreviation \n",
    "    parameters:\n",
    "        word - string, the abbreviation\n",
    "        lang_model - the language model\n",
    "        missed_model - the abbreviation probability model\n",
    "        freedom - possible quality range of log likelihood of the candidates\n",
    "        max_attempts - maximum number of iterations\n",
    "        optimism - coefficient for log likelihood of the word end\n",
    "        verbose - whether to print current candidates in the runtime\n",
    "    returns: dict of keys - suggested phrases, and values - \n",
    "        minus log likelihood of candidates\n",
    "        The less this value, the more likely the suggestion\n",
    "    \"\"\"\n",
    "    query = word + ' '\n",
    "    prefix = ' '\n",
    "    prefix_proba = 0.0\n",
    "    suffix = query\n",
    "    full_origin_logprob = -lang_model.single_log_proba(prefix, query)\n",
    "    no_missing_logprob = -missed_model.single_log_proba(prefix, query)\n",
    "    best_logprob = full_origin_logprob + no_missing_logprob\n",
    "    # add an empty prefix to the heap\n",
    "    heap = [(best_logprob * optimism, prefix, suffix, '', best_logprob * optimism)]\n",
    "    # add the default candidate (without missing characters) \n",
    "    candidates = [(best_logprob, prefix + query, '', None, 0.0)]\n",
    "    if verbose:\n",
    "        print('baseline score is', best_logprob)\n",
    "    # prepare storage of the phrase suffix probabilities\n",
    "    cache = {}\n",
    "    for i in range(len(query)+1):\n",
    "        future_suffix = query[:i]\n",
    "        cache[len(future_suffix)] = -lang_model.single_log_proba('', future_suffix) # rough approximation\n",
    "        cache[len(future_suffix)] += -missed_model.single_log_proba('', future_suffix) # at least add missingness\n",
    "    \n",
    "    for i in range(max_attempts):\n",
    "        if not heap:\n",
    "            break\n",
    "        next_best = heappop(heap)\n",
    "        if verbose:\n",
    "            print(next_best)\n",
    "        if next_best[2] == '':  # the phrase is fully decoded\n",
    "            # if the phrase is good enough, add it to the answer\n",
    "            if next_best[0] <= best_logprob + freedom:\n",
    "                candidates.append(next_best)\n",
    "                # update estimate of the best likelihood\n",
    "                if next_best[0] < best_logprob:\n",
    "                    best_logprob = next_best[0]\n",
    "        else: # # the phrase is not fully decoded - generate more options\n",
    "            prefix_proba = next_best[0] - next_best[4] # all proba estimate minus suffix\n",
    "            prefix = next_best[1]\n",
    "            suffix = next_best[2]\n",
    "            new_options = generate_options(\n",
    "                prefix_proba, prefix, suffix, lang_model, \n",
    "                missed_model, optimism, cache)\n",
    "            # add only the solution potentially no worse than the best + freedom\n",
    "            for new_option in new_options: \n",
    "                if new_option[0] < best_logprob + freedom:\n",
    "                    heappush(heap, new_option)\n",
    "    if verbose:\n",
    "        print('heap size is', len(heap), 'after', i, 'iterations')\n",
    "    result = {}\n",
    "    for candidate in candidates:\n",
    "        if candidate[0] <= best_logprob + freedom:\n",
    "            result[candidate[1][1:-1]] = candidate[0]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "# read the text\n",
    "with open('resource/opensubtitle.txt', encoding = 'utf-8') as f:\n",
    "    text = f.read()\n",
    "# leave only letters and spaces in the text\n",
    "text2 = re.sub(r'[^a-z ]+', '', text.lower().replace('\\n', ' '))\n",
    "all_letters = ''.join(list(sorted(list(set(text2)))))\n",
    "# print(repr(all_letters)) # ' abcdefghijklmnopqrstuvwxyz'\n",
    "# Prepare training sample for the abbreviation model \n",
    "missing_set =  (\n",
    "    [(all_letters, '-' * len(all_letters))] * 3 # all chars missing\n",
    "    + [(all_letters, all_letters)] * 10 # all chars are NOT missing\n",
    "    + [('aeiouy', '------')] * 30 # only vowels are missing\n",
    ")\n",
    "# Train the both models\n",
    "big_lang_m = LanguageNgramModel(order=4, smoothing=0.001, recursive=0.01)\n",
    "big_lang_m.fit(text2)\n",
    "big_err_m = MissingLetterModel(order=0, smoothing_missed=0.1)\n",
    "big_err_m.fit(missing_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -13416.4686358\n",
      "1 -10815.8694602\n",
      "2 -8995.05099957\n",
      "3 -7168.92485596\n",
      "4 -6056.63530163\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    tmp = LanguageNgramModel(i, 0.001, 0.01)\n",
    "    tmp.fit(text2[0:-5000])\n",
    "    print(i, tmp.single_log_proba(' ', text2[-5000:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
