{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document = [\"liburan keluarga di ancol\",\n",
    "            \"lagi nonton konser dengan keluarga\",\n",
    "            \"ancol ramai lagi konser\"]\n",
    "len(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "from preprocess.normalize import Normalize\n",
    "from preprocess.tokenize import Tokenize\n",
    "from preprocess.symspell import SymSpell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dictionary...\n",
      "Processing dictionary...\n",
      "Copied 94811 words to master dictionary...\n",
      "Copied 679534 hashes to master dictionary...\n"
     ]
    }
   ],
   "source": [
    "# initialize\n",
    "normalizer = Normalize()\n",
    "tokenizer = Tokenize()\n",
    "symspell = SymSpell(max_dictionary_edit_distance=3)\n",
    "symspell.load_complete_model_from_json('preprocess\\data\\corpus_complete_model.json', encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do process\n",
    "doc_preprocessed = []\n",
    "\n",
    "for tweet in document:\n",
    "    # normalize\n",
    "    tweet_norm = normalizer.remove_ascii_unicode(tweet)\n",
    "    tweet_norm = normalizer.remove_rt_fav(tweet_norm)\n",
    "    tweet_norm = normalizer.lower_text(tweet_norm)\n",
    "    tweet_norm = normalizer.remove_newline(tweet_norm)\n",
    "    tweet_norm = normalizer.remove_url(tweet_norm)\n",
    "    tweet_norm = normalizer.remove_emoticon(tweet_norm)\n",
    "    tweet_norm = normalizer.remove_hashtag_mention(tweet_norm)\n",
    "    tweet_norm = normalizer.remove_punctuation(tweet_norm)\n",
    "    \n",
    "    # tokenize\n",
    "    tweet_tok = tokenizer.WordTokenize(tweet_norm, removepunct=True)\n",
    "    \n",
    "    # spell correction\n",
    "    temp = []\n",
    "    for token in tweet_tok:\n",
    "        suggestion = symspell.lookup(phrase=token, verbosity=1, max_edit_distance=3)\n",
    "\n",
    "        # option if there is no suggestion\n",
    "        if len(suggestion) > 0:\n",
    "            get_suggestion = str(suggestion[0]).split(':')[0]\n",
    "            temp.append(get_suggestion)\n",
    "        else:\n",
    "            temp.append(token)\n",
    "    tweet_prepared = ' '.join(temp)\n",
    "    \n",
    "    doc_preprocessed.append(tweet_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "liburan keluarga di ancol\n",
      "lagi nonton konser dengan keluarga\n",
      "ancol ramai lagi konser\n"
     ]
    }
   ],
   "source": [
    "for _ in doc_preprocessed:\n",
    "    print(_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HMM POS TAGGER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "from hmmtagger.tagger import MainTagger\n",
    "from tokenization import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize\n",
    "tagger = MainTagger(\"resource/Lexicon.trn\", \"resource/Ngram.trn\", 0, 3, 3, 0, 0, False, 0.2, 0, 500.0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do process\n",
    "doc_tagged = []\n",
    "\n",
    "for tweet in doc_preprocessed:\n",
    "    if len(tweet) == 0: continue\n",
    "    out = sentence_extraction(cleaning(tweet))\n",
    "\n",
    "    join_token = []\n",
    "    for o in out:\n",
    "        strtag = \" \".join(tokenisasi_kalimat(o)).strip()\n",
    "        join_token.extend(tagger.taggingStr(strtag))\n",
    "    doc_tagged.append(' '.join(join_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['liburan/NN keluarga/NN di/IN ancol/NN',\n",
       " 'lagi/RB nonton/NN konser/NN dengan/IN keluarga/NN',\n",
       " 'ancol/NN ramai/JJ lagi/RB konser/NN']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tag Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define\n",
    "Ccon = ['JJ', 'NN','NNP', 'NNG', 'VBI', 'VBT']\n",
    "Cfnc = ['OP', 'CP', 'GM', ';', ':', '\"', '.',\n",
    "         ',', '-', '...', 'RB', 'IN', 'MD', 'CC',\n",
    "         'SC', 'DT', 'UH', 'CDO', 'CDC', 'CDP', 'CDI',\n",
    "         'PRP', 'WP', 'PRN', 'PRL', 'NEG', 'SYM', 'RP', 'FW']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do process\n",
    "doc_classified = []\n",
    "\n",
    "for tweet in doc_tagged:\n",
    "    tweet_split = tweet.split(' ')\n",
    "    \n",
    "    temp = {\"Content\": [], \"Function\": []}\n",
    "    con = []\n",
    "    fnc = []\n",
    "    \n",
    "    for token in tweet_split:\n",
    "        word = token.split('/', 1)[0]\n",
    "        tag = token.split('/', 1)[1]\n",
    "        \n",
    "        if tag in Ccon:\n",
    "            con.append(token)\n",
    "        elif tag in Cfnc:\n",
    "            fnc.append(token)\n",
    "            \n",
    "    temp[\"Content\"].append(' '.join(con))\n",
    "    temp[\"Function\"].append(' '.join(fnc))\n",
    "    \n",
    "    doc_classified.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Content': ['liburan/NN keluarga/NN ancol/NN'], 'Function': ['di/IN']},\n",
       " {'Content': ['nonton/NN konser/NN keluarga/NN'],\n",
       "  'Function': ['lagi/RB dengan/IN']},\n",
       " {'Content': ['ancol/NN ramai/JJ konser/NN'], 'Function': ['lagi/RB']}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['liburan/NN keluarga/NN ancol/NN', 'nonton/NN konser/NN keluarga/NN', 'ancol/NN ramai/JJ konser/NN']\n",
      "\n",
      "[['liburan', 'keluarga', 'ancol'], ['nonton', 'konser', 'keluarga'], ['ancol', 'ramai', 'konser']]\n"
     ]
    }
   ],
   "source": [
    "# split document content and function\n",
    "doc_content = []\n",
    "for tweet in doc_classified:\n",
    "    doc_content.append(''.join(tweet['Content']))\n",
    "\n",
    "print(doc_content)\n",
    "print()\n",
    "\n",
    "# split tag and word\n",
    "doc_prepared = []\n",
    "for tweet in doc_content:\n",
    "    tweet_split = tweet.split(' ')\n",
    "    \n",
    "    temp = []\n",
    "    for token in tweet_split:\n",
    "        word = token.split('/', 1)[0]\n",
    "        temp.append(word)\n",
    "    \n",
    "    doc_prepared.append(temp)\n",
    "\n",
    "print(doc_prepared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "from lda.ldamodel import LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ancol': {0: 1, 1: 0, 2: 1}, 'keluarga': {0: 1, 1: 1, 2: 0}, 'konser': {0: 0, 1: 1, 2: 1}, 'liburan': {0: 1, 1: 0, 2: 0}, 'nonton': {0: 0, 1: 1, 2: 0}, 'ramai': {0: 0, 1: 0, 2: 1}}\n",
      "\n",
      "{'ancol': {0: 0.39723320158102765, 1: 0}, 'keluarga': {0: 0.19960474308300397, 1: 0.24876847290640397}, 'konser': {0: 0, 1: 0.49507389162561577}, 'liburan': {0: 0.19960474308300397, 1: 0}, 'nonton': {0: 0, 1: 0.24876847290640397}, 'ramai': {0: 0.19960474308300397, 1: 0}}\n",
      "\n",
      "{0: {0: 0.9966887417218542, 1: 0}, 1: {0: 0, 1: 0.9966887417218542}, 2: {0: 0.6655629139072847, 1: 0.3344370860927152}}\n",
      "\n",
      "Wall time: 54 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# iniatialize\n",
    "k = 2\n",
    "alpha = 0.01\n",
    "beta = 0.01\n",
    "iterations = 100\n",
    "\n",
    "# do process\n",
    "lda = LdaModel(doc_prepared, k, alpha, beta, iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = lda.get_topic_word_pwz(doc_content)\n",
    "\n",
    "df_lda = pd.DataFrame(result, columns=['Topik', 'Kata', 'PWZ'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topik</th>\n",
       "      <th>Kata</th>\n",
       "      <th>PWZ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ancol/NN</td>\n",
       "      <td>0.397233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>liburan/NN</td>\n",
       "      <td>0.199605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>ramai/JJ</td>\n",
       "      <td>0.199605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>keluarga/NN</td>\n",
       "      <td>0.199605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>konser/NN</td>\n",
       "      <td>0.495074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>keluarga/NN</td>\n",
       "      <td>0.248768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>nonton/NN</td>\n",
       "      <td>0.248768</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topik         Kata       PWZ\n",
       "0      0     ancol/NN  0.397233\n",
       "1      0   liburan/NN  0.199605\n",
       "2      0     ramai/JJ  0.199605\n",
       "3      0  keluarga/NN  0.199605\n",
       "4      1    konser/NN  0.495074\n",
       "5      1  keluarga/NN  0.248768\n",
       "6      1    nonton/NN  0.248768"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.854475187535565"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.perplexity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = np.array([[0.202, 0.00249],\n",
    "              [0.202, 0.252],\n",
    "              [0.401, 0.00249],\n",
    "              [0.002, 0.252],\n",
    "              [0.002, 0.501],\n",
    "              [0.202, 0.00249]])\n",
    "r = np.array([[1, 0.00332, 0.668],\n",
    "              [0.00332, 1, 0.336]])\n",
    "p = np.array([[1, 0, 0],\n",
    "              [1, 1, 0],\n",
    "              [1, 0, 1],\n",
    "              [0, 1, 0],\n",
    "              [0, 1, 1],\n",
    "              [0, 0, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.20200827, 0.00316064, 0.13577264],\n",
       "       [0.20283664, 0.25267064, 0.219608  ],\n",
       "       [0.40100827, 0.00382132, 0.26870464],\n",
       "       [0.00283664, 0.25200664, 0.086008  ],\n",
       "       [0.00366332, 0.50100664, 0.169672  ],\n",
       "       [0.20200827, 0.00316064, 0.13577264]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = np.matmul(q, r)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.69463086, -2.50022497, -0.86718774],\n",
       "       [-0.69285359, -0.59744522, -0.65835184],\n",
       "       [-0.39684667, -2.41778659, -0.57072483],\n",
       "       [-2.54719578, -0.59858802, -1.06546115],\n",
       "       [-2.43612514, -0.30015652, -0.77038982],\n",
       "       [-0.69463086, -2.50022497, -0.86718774]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_s = np.log10(s, out=np.zeros_like(s), where=(s!=0))\n",
    "log_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.69463086, -0.        , -0.        ],\n",
       "       [-0.69285359, -0.59744522, -0.        ],\n",
       "       [-0.39684667, -0.        , -0.57072483],\n",
       "       [-0.        , -0.59858802, -0.        ],\n",
       "       [-0.        , -0.30015652, -0.77038982],\n",
       "       [-0.        , -0.        , -0.86718774]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_log_s = np.multiply(p, log_s)\n",
    "p_log_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_p = np.sum(p)\n",
    "sum_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.488823270271718"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_p_log_s = np.sum(p_log_s)\n",
    "sum_p_log_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8401907822520358"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity = np.exp(-(sum_p_log_s/sum_p))\n",
    "perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_perplexity = [0 for x in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
