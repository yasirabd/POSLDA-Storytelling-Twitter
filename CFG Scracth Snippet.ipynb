{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict, defaultdict\n",
    "import re\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {0: [\"shelter/NN\", \"naik/VBI\", \"turun/VBI\", \"kampung/NN\", \"pantai/NN\", \"marina/NN\", \"pelangi/NN\", \"jalan/NN\", \"simpang/NN\", \"deket/JJ\"],\n",
    "        1: [\"simpang/NN\", \"arus/NN\", \"menit/NN\", \"lintas/NN\", \"lalu/JJ\", \"tugu/NN\", \"muda/JJ\", \"parkir/NN\", \"kawasan/NN\", \"bawah/NN\"],\n",
    "        2: [\"semarang/NN\", \"simpang/NN\", \"bhayangkara/NN\", \"lapangan/NN\", \"jateng/NN\", \"hut/NN\", \"daerah/NN\", \"gue/NN\", \"hari/NN\", \"rakyat/NN\"],\n",
    "        3: [\"semarang/NN\", \"canyon/NN\", \"brown/NN\", \"simpang/NN\", \"pak/NN\", \"melalui/VBT\", \"lokasi/NN\", \"kota/NN\", \"sih/NN\", \"jasa/NN\"],\n",
    "        4: [\"lawang/NN\", \"sewu/NN\", \"sama/JJ\", \"foto/NN\", \"jadi/JJ\", \"film/NN\", \"orang/NN\", \"nonton/NN\", \"resa/NN\", \"depan/NN\"],\n",
    "        5: [\"lawang/NN\", \"sewu/NN\", \"semarang/NN\", \"jawa/NN\", \"tengah/NN\", \"indonesia/NN\", \"api/NN\", \"kereta/NN\", \"kantor/NN\", \"ada/VBI\"],\n",
    "        6: [\"bawah/NN\", \"tempat/NN\", \"lantai/NN\", \"tanah/NN\", \"ruang/NN\", \"utama/JJ\", \"basement/NN\", \"air/NN\", \"guide/NN\", \"depan/NN\"],\n",
    "        7: [\"sewu/NN\", \"lawang/NN\", \"dibangun/VBT\", \"tahun/NN\", \"bangunan/NN\", \"belanda/NN\", \"memiliki/VBT\", \"ning/NNP\", \"salah/JJ\", \"aku/NN\"],\n",
    "        8: [\"kota/NN\", \"koridor/NN\", \"simpang/NN\", \"semarang/NN\", \"sewu/NN\", \"lawang/NN\", \"lama/JJ\", \"sam/NN\", \"kong/NN\", \"poo/NN\"],\n",
    "        9: [\"simpang/NN\", \"kediri/NN\", \"gumul/NN\", \"kabupaten/NN\", \"budaya/NN\", \"pekan/NN\", \"jalan/NN\", \"pariwisata/NN\", \"radio/NN\", \"monumen/NN\"],\n",
    "        10: [\"pecinan/NN\", \"kota/NN\", \"rumah/NN\", \"magelang/NN\", \"semanis/NN\", \"jam/NN\", \"aja/NN\", \"salah/JJ\", \"gak/NN\", \"keep/NN\"]\n",
    "       }"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "grammar = {\n",
    "    \"_S\"  : [\"_NP _VP\"],\n",
    "    \"_NP\" : [\"_NN\",\"_NNP\", \"_NNG\", \"_NN _DT\",\n",
    "             \"_NN _JJP\", \"_NNP _JJP\", \"_NP _CC _NP\"],\n",
    "    \"_VP\" : [\"_VBT _NP\", \"_VBT _NP _PP\", \"_VBI\",\n",
    "             \"_VBI _NP\", \"_PP\", \"_JJP\"],\n",
    "    \"_PP\" : [\"_IN _NP\"],\n",
    "    \"_JJP\": [\"_JJ\", \"_JJ _CC _JJ\"],\n",
    "    \"_CC\" : [\"dan\", \"atau\"],\n",
    "    \"_IN\" : [\"di\", \"ke\", \"dari\"],\n",
    "    \"_DT\" : [\"ini\", \"itu\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NP = ['_NN', '_NNP', '_NNG', '_NN _DT', '_DT _NNP', '_DT _NNG', '_NN _JJP', '_NNP _JJP', '_NP _CC _NP']\n",
    "VP = ['_VBI _NP', '_VBI _IN _NP', '_VBI _NP _PP', '_VBT', '_VBT _NP', '_PP', '_JJP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def organize_words_by_tag(dict_words_by_topic):\n",
    "    '''organize words by tag'''\n",
    "    result = defaultdict(list)\n",
    "    i = []\n",
    "    for s in dict_words_by_topic:\n",
    "        word = s.split('/')[0]\n",
    "        tag = s.split('/')[1]\n",
    "        result['_'+tag].append(word)\n",
    "    return dict(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_story_tag(dict_words_by_tag):\n",
    "    '''Get tags in dictionary contains tag and words'''\n",
    "    result = []\n",
    "    for key in dict_words_by_tag:\n",
    "        result.append(key)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_base_grammar(list_tag):\n",
    "    result = {}\n",
    "    S = {\"_S\": [\"_NP _VP\"]}\n",
    "    PP = {\"_PP\": [\"_IN _NP\"]}\n",
    "    if '_JJ' in list_tag:\n",
    "        NP_RULES = ['_NP _CC _NP'] + generate_NP(list_tag)\n",
    "        NP = {\"_NP\": NP_RULES}\n",
    "        if check_VP(list_tag):\n",
    "            VP_RULES = ['_PP', '_JJP'] + generate_VP(list_tag)\n",
    "            VP = {\"_VP\": VP_RULES}\n",
    "        else:\n",
    "            VP = {\"_VP\": ['_PP', '_JJP']}\n",
    "        JJP = {\"_JJP\": ['_JJ', '_JJ _CC _JJ']}\n",
    "        \n",
    "        for r in [S, NP, VP, PP, JJP]:\n",
    "            result.update(r)\n",
    "        return result\n",
    "    else:\n",
    "        NP_RULES = ['_NP _CC _NP'] + remove_JJP(generate_NP(list_tag))\n",
    "        NP = {\"_NP\": NP_RULES}\n",
    "        if check_VP(list_tag):\n",
    "            VP_RULES = ['_PP'] + remove_JJP(generate_VP(list_tag))\n",
    "            VP = {\"_VP\": VP_RULES}\n",
    "        else:\n",
    "            VP = {\"_VP\": ['_PP']}\n",
    "        \n",
    "        for r in [S, NP, VP, PP]:\n",
    "            result.update(r)\n",
    "        return result\n",
    "        \n",
    "def check_VP(list_tag):\n",
    "    for tag in list_tag:\n",
    "        if '_V' in tag:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def remove_JJP(list_tag):\n",
    "    result = []\n",
    "    for tag in list_tag:\n",
    "        if '_JJ' in tag:\n",
    "            continue\n",
    "        else:\n",
    "            result.append(tag)\n",
    "    return result\n",
    "    \n",
    "\n",
    "def generate_NP(list_tag):\n",
    "    result = []\n",
    "    for tag in list_tag:\n",
    "        for words in NP:\n",
    "            if re.search(r'\\b' + tag + r'\\b', words):\n",
    "                result.append(words)\n",
    "    return list(OrderedDict.fromkeys(result))\n",
    "\n",
    "def generate_VP(list_tag):\n",
    "    result = []\n",
    "    for tag in list_tag:\n",
    "        for words in VP:\n",
    "            if re.search(r'\\b' + tag + r'\\b', words):\n",
    "                result.append(words)\n",
    "    return list(OrderedDict.fromkeys(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_words_grammar(dict_words_by_tag):\n",
    "    result = {}\n",
    "    IN = {\"_IN\": ['di', 'ke', 'dari']}\n",
    "    DT = {\"_DT\": ['ini', 'itu']}\n",
    "    CC = {\"_CC\": ['dan', 'atau']}\n",
    "    WORDS = dict_words_by_tag\n",
    "    for r in [IN, DT, CC, WORDS]:\n",
    "        result.update(r)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_grammar(dict_words_by_topic):\n",
    "    result = {}\n",
    "    dict_words_by_tag = organize_words_by_tag(dict_words_by_topic)\n",
    "    list_tag = get_story_tag(dict_words_by_tag)\n",
    "    \n",
    "    base_grammar = generate_base_grammar(list_tag)\n",
    "    words_grammar = generate_words_grammar(dict_words_by_tag)\n",
    "    for r in [base_grammar, words_grammar]:\n",
    "        result.update(r)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def is_terminal(token):\n",
    "    return token[0] != \"_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand(grammar, tokens):\n",
    "    for i, token in enumerate(tokens):\n",
    "\n",
    "        # skip over terminals\n",
    "        if is_terminal(token): continue\n",
    "\n",
    "        # if we get here, we found a non-terminal token\n",
    "        # so we need to choose a replacement at random\n",
    "        replacement = random.choice(grammar[token])\n",
    "\n",
    "        if is_terminal(replacement):\n",
    "            tokens[i] = replacement\n",
    "        else:\n",
    "            tokens = tokens[:i] + replacement.split() + tokens[(i+1):]\n",
    "\n",
    "        # now call expand on the new list of tokens\n",
    "        return expand(grammar, tokens)\n",
    "\n",
    "    # if we get here we had all terminals and are done\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(grammar):\n",
    "    return expand(grammar, [\"_S\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jalan', 'deket', 'atau', 'deket', 'atau', 'shelter', 'deket', 'deket']\n",
      "['simpang', 'turun', 'kampung', 'deket', 'ke', 'simpang', 'deket', 'atau', 'deket']\n",
      "['marina', 'deket', 'naik', 'pelangi', 'dan', 'kampung', 'itu', 'ke', 'shelter']\n",
      "['pelangi', 'deket', 'naik', 'shelter', 'deket', 'dari', 'simpang', 'deket', 'atau', 'deket', 'dan', 'shelter', 'itu']\n",
      "['kampung', 'dan', 'jalan', 'deket', 'atau', 'deket']\n",
      "['pantai', 'turun', 'pantai', 'ini', 'dari', 'jalan', 'itu']\n",
      "['pantai', 'itu', 'naik', 'kampung', 'itu', 'ke', 'pelangi', 'itu']\n",
      "['shelter', 'deket', 'atau', 'shelter', 'atau', 'kampung', 'deket', 'dan', 'deket', 'turun', 'pelangi', 'deket', 'atau', 'deket']\n",
      "['pantai', 'deket', 'atau', 'deket', 'turun', 'ke', 'pelangi', 'deket', 'dan', 'deket']\n",
      "['pelangi', 'turun', 'simpang', 'deket', 'atau', 'deket']\n"
     ]
    }
   ],
   "source": [
    "grammar = create_grammar(data[0])\n",
    "for i in range(10):\n",
    "    print(generate_sentence(grammar))\n",
    "#     print(' '.join(generate_sentence(grammar)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_two_dicts(x, y):\n",
    "    \"\"\"Given two dicts, merge them into a new dict as a shallow copy.\"\"\"\n",
    "    z = x.copy()\n",
    "    z.update(y)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentence(data):\n",
    "    result = {}\n",
    "    for topic, words in data.items():\n",
    "        sentence = []\n",
    "        grammar = create_grammar(words)\n",
    "        for s in range(10):\n",
    "            sentence.append(' '.join(generate_sentence(grammar)))\n",
    "        result = merge_two_dicts(result, {topic: sentence})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "a = create_sentence(data)\n",
    "print(type(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
