{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "Hal yang dilakukan:\n",
    "1. Common Preprocessing\n",
    "    1. Remove ASCII and Unicode.\n",
    "    2. Tokenization\n",
    "    3. Case folding, convert into lowercase\n",
    "    4. Repeated dot (sedih... -> sedih.)\n",
    "    5. Repeated character ('hehe :)))' -> 'hehe :)')\n",
    "    6. Remove elipsis (lanjut baca... -> lanjut baca)\n",
    "    7. Repeated word that has meaning ('malam malam' -> 'malam-malam')\n",
    "    8. Remove newline\n",
    "2. Specific Preprocessing\n",
    "    1. Special symbols on Twitter, removing hashtag, mention, RT, and FAV\n",
    "    2. Remove all emoticons\n",
    "    3. Remove URL\n",
    "    4. Spell checker using noisy channel approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(236, 6)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets = pd.read_csv('export.csv')\n",
    "df_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>username</th>\n",
       "      <th>created_at</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.670767e+08</td>\n",
       "      <td>Rosiwulan_</td>\n",
       "      <td>2018-03-28 00:24:35</td>\n",
       "      <td>-7.64800</td>\n",
       "      <td>111.319250</td>\n",
       "      <td>At Sam Poo Kong Temple (Zheng He Temple) [pic]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.685386e+07</td>\n",
       "      <td>dodydokong</td>\n",
       "      <td>2018-03-28 00:21:39</td>\n",
       "      <td>-6.99630</td>\n",
       "      <td>110.398000</td>\n",
       "      <td>Sam Poo Kong\\n\\nm Poo Kong (Chinese: 三保洞; piny...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.424832e+09</td>\n",
       "      <td>ATCSKotaSMG</td>\n",
       "      <td>2018-03-27 03:00:26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.00 WIB Lalin Simp Sam Poo Kong terpantau ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.099801e+08</td>\n",
       "      <td>deadheblonk</td>\n",
       "      <td>2018-03-26 18:02:33</td>\n",
       "      <td>-7.01141</td>\n",
       "      <td>110.386012</td>\n",
       "      <td>Just posted a photo @ Wisata Sam Poo Kong Sema...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.424832e+09</td>\n",
       "      <td>ATCSKotaSMG</td>\n",
       "      <td>2018-03-26 03:05:36</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.05 WIB SAM POO KONG\\nLalin terpantau relati...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id     username           created_at  latitude   longitude  \\\n",
       "0  2.670767e+08   Rosiwulan_  2018-03-28 00:24:35  -7.64800  111.319250   \n",
       "1  8.685386e+07   dodydokong  2018-03-28 00:21:39  -6.99630  110.398000   \n",
       "2  2.424832e+09  ATCSKotaSMG  2018-03-27 03:00:26       NaN         NaN   \n",
       "3  2.099801e+08  deadheblonk  2018-03-26 18:02:33  -7.01141  110.386012   \n",
       "4  2.424832e+09  ATCSKotaSMG  2018-03-26 03:05:36       NaN         NaN   \n",
       "\n",
       "                                                text  \n",
       "0  At Sam Poo Kong Temple (Zheng He Temple) [pic]...  \n",
       "1  Sam Poo Kong\\n\\nm Poo Kong (Chinese: 三保洞; piny...  \n",
       "2  10.00 WIB Lalin Simp Sam Poo Kong terpantau ra...  \n",
       "3  Just posted a photo @ Wisata Sam Poo Kong Sema...  \n",
       "4  10.05 WIB SAM POO KONG\\nLalin terpantau relati...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2018-03-28', '00:24:35']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets['created_at'][0].split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2018-03-17', '10:48:33.156579']\n",
      "2018-03-28 00:24:35\n",
      "2018-06-25 10:48:33.156579\n",
      "2018-06-25 10:48:33.156579\n",
      "2018-06-15 10:48:33.157579\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "datetime_object = datetime.strptime(df_tweets['created_at'][0], '%Y-%m-%d %H:%M:%S')\n",
    "d = datetime.today() - timedelta(days=100, microseconds=0)\n",
    "n = datetime.today()\n",
    "nn = datetime.today()\n",
    "print(str(d).split(' '))\n",
    "print(datetime_object)\n",
    "print(n)\n",
    "print(nn)\n",
    "now = datetime.today()\n",
    "date_before = now - timedelta(days=10)\n",
    "print(date_before)\n",
    "if n<nn:\n",
    "    print('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modulenorm.Normalize import Normalize\n",
    "from modulenorm.Tokenize import Tokenize\n",
    "from modulenorm.SymSpell import SymSpell\n",
    "from modulenorm.SymSpell2 import SymSpell2\n",
    "from modulenorm.LanguageNgramModel import LanguageNgramModel\n",
    "from modulenorm.MissingLetterModel import MissingLetterModel\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['malam-malam', 'gini', 'enaknya', 'ngapain', 'ya.']\n"
     ]
    }
   ],
   "source": [
    "tok = Tokenize()\n",
    "toktext = tok.WordTokenize('malam-malam gini enaknya ngapain ya.')\n",
    "print(toktext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def _clean_text(text):\n",
    "        '''Remove unwanted characters and extra spaces from the text'''\n",
    "        text = re.sub(\" [b-zB-Z] \", ' ', text) #except a or A remove all single char words\n",
    "#         text = re.sub('[^0-9a-zA-Z]+', ' ', text) #remove all non alpha numeric chars\n",
    "        text = re.sub('[ \\t]+', ' ', text) #remove continuous space/tabs\n",
    "        text = re.sub(r'\\n', ' ', text) \n",
    "        text = re.sub(r'[{}_*>()\\\\%+=\\[\\]]','', text)\n",
    "        text = re.sub('a0','', text)\n",
    "        text = re.sub('\\'92t','\\'t', text)\n",
    "        text = re.sub('\\'92s','\\'s', text)\n",
    "        text = re.sub('\\'92m','\\'m', text)\n",
    "        text = re.sub('\\'92ll','\\'ll', text)\n",
    "        text = re.sub('\\'91','', text)\n",
    "        text = re.sub('\\'92','', text)\n",
    "        text = re.sub('\\'93','', text)\n",
    "        text = re.sub('\\'94','', text)\n",
    "        text = re.sub('\\.','. ', text)\n",
    "        text = re.sub('\\!','! ', text)\n",
    "        text = re.sub('\\?','? ', text)\n",
    "        text = re.sub(' +',' ', text)\n",
    "        text = re.sub('\\s+',' ', text)\n",
    "        text = re.sub('[0-9]+','', text)\n",
    "        try:\n",
    "            text1=unidecode(str(text))\n",
    "        except:\n",
    "            return text\n",
    "    \n",
    "        return text1\n",
    "    \n",
    "def remove_hashmention(text):\n",
    "    result = []\n",
    "    text = text.split(' ')\n",
    "    for t in text:\n",
    "        if t.startswith('#') or t.startswith('@'):\n",
    "            continue\n",
    "        else:\n",
    "            result.append(t)\n",
    "    return ' '.join(result)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    remove = string.punctuation\n",
    "    remove = remove.replace(\"-\", \"\")\n",
    "    translator = str.maketrans(remove, ' '*len(remove))\n",
    "    return text.translate(translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rencang muda tempat nongkrong yang ramai di boyolali  menurut kalian di mana  alun-alun simpang lima atau di mana    lanjut baca  \n"
     ]
    }
   ],
   "source": [
    "row = \"Rencang Muda,tempat nongkrong yang ramai di boyolali  menurut kalian di mana??alun-alun,simpang lima atau di mana?? :))) lanjut baca..\"\n",
    "# normalize\n",
    "norm = Normalize()\n",
    "text_norm = norm.remove_ascii_unicode(row)\n",
    "text_norm = norm.remove_rt_fav(text_norm)\n",
    "text_norm = norm.lower_text(text_norm)\n",
    "# text_norm = norm.repeat_char_modify(text_norm)\n",
    "# text_norm = norm.remove_elipsis(text_norm)\n",
    "text_norm = norm.remove_newline(text_norm)\n",
    "text_norm = norm.remove_url(text_norm)\n",
    "text_norm = norm.remove_emoticons(text_norm)\n",
    "# text_norm = norm.remove_hashtags_mentions(text_norm)\n",
    "text_norm = remove_hashmention(text_norm)\n",
    "text_norm = remove_punctuation(text_norm)\n",
    "\n",
    "print(text_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rencang', 'muda', 'tempat', 'nongkrong', 'yang', 'ramai', 'di', 'boyolali', 'menurut', 'kalian', 'di', 'mana', 'alun-alun', 'simpang', 'lima', 'atau', 'di', 'mana', 'lanjut', 'baca']\n"
     ]
    }
   ],
   "source": [
    "toktext = tok.WordTokenize(text_norm, removepunct=True)\n",
    "print(toktext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = SymSpell2(max_dictionary_edit_distance=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dictionary...\n",
      "Processing dictionary...\n",
      "Copied 94815 words to master dictionary...\n",
      "Copied 679555 hashes to master dictionary...\n"
     ]
    }
   ],
   "source": [
    "ss.load_complete_model_from_json(\"resource/corpus_complete_model.json\",encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rencang muda tempat nongkrong yang ramai di boyolali menurut kalian di mana alun-alun simpang lima atau di mana\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "for t in toktext:\n",
    "    suggestion_list = ss.lookup(phrase=t, verbosity=1, max_edit_distance=2)\n",
    "    if len(suggestion_list) > 0:\n",
    "        result.append(str(suggestion_list[0]).split(':')[0])\n",
    "    else:\n",
    "        result.append(t)\n",
    "\n",
    "print(' '.join(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vavav']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\yasir\\anaconda3\\lib\\re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "words = \"a vavav\"\n",
    "s = words = re.split('(\\w+)?', words)\n",
    "s = [w.strip().lower() for w in words if w.strip() and len(w)>1]\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Train Noisy Channel\n",
    "with open('resource/opensubtitle.txt', encoding = 'utf-8') as f:\n",
    "    text_id = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leave only letters and spaces in the text\n",
    "text_id2 = re.sub(r'[^a-z ]+', '', text_id.lower().replace('\\n', ' '))\n",
    "all_letters = ''.join(list(sorted(list(set(text_id2)))))\n",
    "print(repr(all_letters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training sample for the abbreviation model \n",
    "missing_set =  (\n",
    "    [(all_letters, '-' * len(all_letters))] * 3 # all chars missing\n",
    "    + [(all_letters, all_letters)] * 10 # all chars are NOT missing\n",
    "    + [('aeiouy', '------')] * 30 # only vowels are missing\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Train the both models\n",
    "big_lang_m = LanguageNgramModel(order=4, smoothing=0.001, recursive=0.01)\n",
    "big_lang_m.fit(text_id2)\n",
    "big_err_m = MissingLetterModel(order=0, smoothing_missed=0.1)\n",
    "big_err_m.fit(missing_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# without noisy channel\n",
    "idx = 0\n",
    "df_tweets['normalize'] = None\n",
    "for row in df_tweets['text']:\n",
    "    start_tweet = time.clock()\n",
    "    # normalize\n",
    "    norm = Normalize()\n",
    "    text_norm = norm.remove_ascii_unicode(row)\n",
    "    text_norm = norm.remove_rt_fav(text_norm)\n",
    "    text_norm = norm.lower_text(text_norm)\n",
    "    text_norm = norm.repeat_char_modify(text_norm)\n",
    "    text_norm = norm.remove_elipsis(text_norm)\n",
    "    text_norm = norm.remove_newline(text_norm)\n",
    "    text_norm = norm.remove_url(text_norm)\n",
    "    text_norm = norm.remove_emoticons(text_norm)\n",
    "    text_norm = norm.remove_hashtags_mentions(text_norm)\n",
    "    \n",
    "    # tokenize\n",
    "    tok = Tokenize()\n",
    "    text_norm = tok.WordTokenize(text_norm)\n",
    "    \n",
    "    # spell correction\n",
    "    temp_sentence = []\n",
    "    for token in text_norm:\n",
    "        choosen_word = symspell.get_suggestions(token)\n",
    "        \n",
    "        # option if there is no suggestions\n",
    "        if len(choosen_word) > 0:\n",
    "            temp_sentence.append(choosen_word)\n",
    "        else:\n",
    "            temp_sentence.append(token)\n",
    "    \n",
    "    text_norm = ' '.join(temp_sentence)\n",
    "    df_tweets['normalize'][idx] = text_norm\n",
    "    print('tweets', idx, 'selesai', time.clock()-start_tweet, 'seconds')\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "idx = 0\n",
    "df_tweets['normalize'] = None\n",
    "for row in df_tweets['text']:\n",
    "    start_tweet = time.clock()\n",
    "    # normalize\n",
    "    norm = Normalize()\n",
    "    text_norm = norm.remove_ascii_unicode(row)\n",
    "    text_norm = norm.remove_rt_fav(text_norm)\n",
    "    text_norm = norm.lower_text(text_norm)\n",
    "    text_norm = norm.repeat_char_modify(text_norm)\n",
    "    text_norm = norm.remove_elipsis(text_norm)\n",
    "    text_norm = norm.remove_newline(text_norm)\n",
    "    text_norm = norm.remove_url(text_norm)\n",
    "    text_norm = norm.remove_emoticons(text_norm)\n",
    "    text_norm = norm.remove_hashtags_mentions(text_norm)\n",
    "    \n",
    "    # tokenize\n",
    "    tok = Tokenize()\n",
    "    text_norm = tok.WordTokenize(text_norm)\n",
    "    \n",
    "    # spell correction\n",
    "    temp_sentence = []\n",
    "    for token in text_norm:\n",
    "        if len(token) <= 3 and (not(any(char.isdigit() for char in token))):\n",
    "            nc = norm.noisy_channel(token, big_lang_m, big_err_m)\n",
    "            max_values = max(nc.values())\n",
    "            choosen_word = list(nc.keys())[list(nc.values()).index(max_values)]\n",
    "        else:\n",
    "            choosen_word = symspell.get_suggestions(token)\n",
    "        \n",
    "        # option if there is no sugestions\n",
    "        if len(choosen_word) > 0:\n",
    "            temp_sentence.append(choosen_word)\n",
    "        else:\n",
    "            temp_sentence.append(token)\n",
    "    \n",
    "    text_norm = ' '.join(temp_sentence)\n",
    "    df_tweets['normalize'][idx] = text_norm\n",
    "    print('tweets', idx, 'selesai', time.clock()-start_tweet, 'seconds')\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets['normalize'].to_csv('normalize_export.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tweets = [[1, 'afaf'], [2, 'fafasfba'], [3, 'fasf kvams']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "for lt in list_tweets:\n",
    "    print(lt[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 'afaf'],\n",
       " [2, 'fafasfba'],\n",
       " [3, 'fasf kvams'],\n",
       " [[4, 'afafzzz'], [5, 'faaazzfasfba']],\n",
       " [[4, 'afafzzz'], [5, 'faaazzfasfba']],\n",
       " [[4, 'afafzzz'], [5, 'faaazzfasfba']],\n",
       " [1, 'faf']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bla = [1, 'faf']\n",
    "list_tweets.append(bla)\n",
    "list_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tweets.append(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 'afaf'],\n",
       " [2, 'fafasfba'],\n",
       " [3, 'fasf kvams'],\n",
       " [[4, 'afafzzz'], [5, 'faaazzfasfba']],\n",
       " [[4, 'afafzzz'], [5, 'faaazzfasfba']],\n",
       " [[4, 'afafzzz'], [5, 'faaazzfasfba']]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
