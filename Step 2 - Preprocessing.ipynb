{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "Hal yang dilakukan:\n",
    "1. Common Preprocessing\n",
    "    1. Remove ASCII and Unicode.\n",
    "    2. Tokenization\n",
    "    3. Case folding, convert into lowercase\n",
    "    4. Repeated dot (sedih... -> sedih.)\n",
    "    5. Repeated character ('hehe :)))' -> 'hehe :)')\n",
    "    6. Remove elipsis (lanjut baca... -> lanjut baca)\n",
    "    7. Repeated word that has meaning ('malam malam' -> 'malam-malam')\n",
    "    8. Remove newline\n",
    "2. Specific Preprocessing\n",
    "    1. Special symbols on Twitter, removing hashtag, mention, RT, and FAV\n",
    "    2. Remove all emoticons\n",
    "    3. Remove URL\n",
    "    4. Spell checker using noisy channel approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets = pd.read_csv('export.csv')\n",
    "df_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets['created_at'][0].split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "datetime_object = datetime.strptime(df_tweets['created_at'][0], '%Y-%m-%d %H:%M:%S')\n",
    "d = datetime.today() - timedelta(days=100, microseconds=0)\n",
    "n = datetime.today()\n",
    "nn = datetime.today()\n",
    "print(str(d).split(' '))\n",
    "print(datetime_object)\n",
    "print(n)\n",
    "print(nn)\n",
    "now = datetime.today()\n",
    "date_before = now - timedelta(days=10)\n",
    "print(date_before)\n",
    "if n<nn:\n",
    "    print('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modulenorm.Normalize import Normalize\n",
    "from modulenorm.Tokenize import Tokenize\n",
    "from modulenorm.SymSpell import SymSpell\n",
    "from modulenorm.SymSpell2 import SymSpell2\n",
    "from modulenorm.LanguageNgramModel import LanguageNgramModel\n",
    "from modulenorm.MissingLetterModel import MissingLetterModel\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['faksfnsafk', 'knaskvna', 'msalkfm', 'askfmlas', 'kvadnklcasc']\n"
     ]
    }
   ],
   "source": [
    "tok = Tokenize()\n",
    "toktext = tok.WordTokenize('faksfnSAFk knaskvna msalkfm askfmlas #kvadnklcasc#%#%.', removepunct=True)\n",
    "print(toktext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# SymSpell Create Dictionary\n",
    "symspell2 = SymSpell2()\n",
    "symspell2.create_dictionary(\"resource/kamus.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "symspell2 = SymSpell2()\n",
    "a = symspell2.clean_and_create_dictionary(\"resource/kamus.txt\")\n",
    "with open('file2.txt', 'w') as file:\n",
    "     file.write(json.dumps(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "a = symspell.create_dictionary(\"resource/novel.txt\")\n",
    "f = open(\"file.pkl\",\"wb\")\n",
    "pickle.dump(a, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(symspell))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Train Noisy Channel\n",
    "with open('resource/opensubtitle.txt', encoding = 'utf-8') as f:\n",
    "    text_id = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leave only letters and spaces in the text\n",
    "text_id2 = re.sub(r'[^a-z ]+', '', text_id.lower().replace('\\n', ' '))\n",
    "all_letters = ''.join(list(sorted(list(set(text_id2)))))\n",
    "print(repr(all_letters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training sample for the abbreviation model \n",
    "missing_set =  (\n",
    "    [(all_letters, '-' * len(all_letters))] * 3 # all chars missing\n",
    "    + [(all_letters, all_letters)] * 10 # all chars are NOT missing\n",
    "    + [('aeiouy', '------')] * 30 # only vowels are missing\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Train the both models\n",
    "big_lang_m = LanguageNgramModel(order=4, smoothing=0.001, recursive=0.01)\n",
    "big_lang_m.fit(text_id2)\n",
    "big_err_m = MissingLetterModel(order=0, smoothing_missed=0.1)\n",
    "big_err_m.fit(missing_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# without noisy channel\n",
    "idx = 0\n",
    "df_tweets['normalize'] = None\n",
    "for row in df_tweets['text']:\n",
    "    start_tweet = time.clock()\n",
    "    # normalize\n",
    "    norm = Normalize()\n",
    "    text_norm = norm.remove_ascii_unicode(row)\n",
    "    text_norm = norm.remove_rt_fav(text_norm)\n",
    "    text_norm = norm.lower_text(text_norm)\n",
    "    text_norm = norm.repeat_char_modify(text_norm)\n",
    "    text_norm = norm.remove_elipsis(text_norm)\n",
    "    text_norm = norm.remove_newline(text_norm)\n",
    "    text_norm = norm.remove_url(text_norm)\n",
    "    text_norm = norm.remove_emoticons(text_norm)\n",
    "    text_norm = norm.remove_hashtags_mentions(text_norm)\n",
    "    \n",
    "    # tokenize\n",
    "    tok = Tokenize()\n",
    "    text_norm = tok.WordTokenize(text_norm)\n",
    "    \n",
    "    # spell correction\n",
    "    temp_sentence = []\n",
    "    for token in text_norm:\n",
    "        choosen_word = symspell.get_suggestions(token)\n",
    "        \n",
    "        # option if there is no suggestions\n",
    "        if len(choosen_word) > 0:\n",
    "            temp_sentence.append(choosen_word)\n",
    "        else:\n",
    "            temp_sentence.append(token)\n",
    "    \n",
    "    text_norm = ' '.join(temp_sentence)\n",
    "    df_tweets['normalize'][idx] = text_norm\n",
    "    print('tweets', idx, 'selesai', time.clock()-start_tweet, 'seconds')\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "idx = 0\n",
    "df_tweets['normalize'] = None\n",
    "for row in df_tweets['text']:\n",
    "    start_tweet = time.clock()\n",
    "    # normalize\n",
    "    norm = Normalize()\n",
    "    text_norm = norm.remove_ascii_unicode(row)\n",
    "    text_norm = norm.remove_rt_fav(text_norm)\n",
    "    text_norm = norm.lower_text(text_norm)\n",
    "    text_norm = norm.repeat_char_modify(text_norm)\n",
    "    text_norm = norm.remove_elipsis(text_norm)\n",
    "    text_norm = norm.remove_newline(text_norm)\n",
    "    text_norm = norm.remove_url(text_norm)\n",
    "    text_norm = norm.remove_emoticons(text_norm)\n",
    "    text_norm = norm.remove_hashtags_mentions(text_norm)\n",
    "    \n",
    "    # tokenize\n",
    "    tok = Tokenize()\n",
    "    text_norm = tok.WordTokenize(text_norm)\n",
    "    \n",
    "    # spell correction\n",
    "    temp_sentence = []\n",
    "    for token in text_norm:\n",
    "        if len(token) <= 3 and (not(any(char.isdigit() for char in token))):\n",
    "            nc = norm.noisy_channel(token, big_lang_m, big_err_m)\n",
    "            max_values = max(nc.values())\n",
    "            choosen_word = list(nc.keys())[list(nc.values()).index(max_values)]\n",
    "        else:\n",
    "            choosen_word = symspell.get_suggestions(token)\n",
    "        \n",
    "        # option if there is no sugestions\n",
    "        if len(choosen_word) > 0:\n",
    "            temp_sentence.append(choosen_word)\n",
    "        else:\n",
    "            temp_sentence.append(token)\n",
    "    \n",
    "    text_norm = ' '.join(temp_sentence)\n",
    "    df_tweets['normalize'][idx] = text_norm\n",
    "    print('tweets', idx, 'selesai', time.clock()-start_tweet, 'seconds')\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets['normalize'].to_csv('normalize_export.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
