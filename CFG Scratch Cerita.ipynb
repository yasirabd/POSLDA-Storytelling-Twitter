{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "document1 = [\"Kalian inget weekend kemarin saya ke Lawang-Sewu Semarang berbarengan @Jaktvalken @si_wel kan? #memetwit\",\n",
    "            \"Gw pagi sampai di Semarang, ketemuan berbarengan @Jaktvalken berbarengan @si_wel di simpang lima, trus sarapan gudeg. Lanjut deh tuh ke Lawang-Sewu betiga\",\n",
    "            \"sampai di Lawang-Sewu, gw udah excited karena memang hobi memotret, setiap sudutnya tuh instagramable banget, dan gw ga ada curiga apapun disitu\",\n",
    "            \"Gw berkeliling ke berbagai sudut ruangan, gw yang memang ga terlalu peka ngerasa biasa, asik memotret sana sini sambil sesekali bergaya betiga\",\n",
    "            \"sampai ketika kami bertiga duduk persis disamping ini, fox @Jaktvalken ngomong 'om ada yang ngelihatin kita dari tadi, di lantai 2'\",\n",
    "            \"Gw mencoba biasa dan cuek, dan kami rencananya mau ke basement bekas penjara bawah tanah jaman dulu, tp sehabis istirahat dulu, capek\",\n",
    "            \"Kami lanjut nyari jalan akses ke basement, tapi sayangnya wisata bawah tanah / basement lagi ditutup. Yaudah kan kami keliling lagi\",\n",
    "            \"Kami berkeliling ke lantai 2, lantai 3, semua dijelajah, pelampiasan karena ke basement ditutup. Ini siang hari, memang suasananya gitu\",\n",
    "            \"Ini foto2 di lantai 3, gw rasa lu bisa lihat sesuatu selain kita bertiga di foto2 ini, Entah lihat sendiri :|\",\n",
    "            \"@si_wel sehabis foto2 itu kita turun, udah capek juga, dan panas banget, pas mau udahan @Jaktvalken request 'om, kita ke pintu akses basement lagi yuk'\",\n",
    "            \"Lagi-lagi gw masih ga ada curiga apa-apa, kita balik ke pintu akses ke ruang bawah tanah, kan memang gw yang bawa kamera, dan kemana2 gw yang memotret\",\n",
    "            \"Tapi tiba-tiba pas memotret di depan sini, seinget gw. Gw baru memotret sekali atau dua kali ini, tiba2 gw mual pengen muntah, gw kabur ke toilet, lemes\",\n",
    "            \"Ada kali gw hilang 10 menit, Aji berbarengan Fox nyari gw, sampai gw kembali ke situ udah lemes, kamera gw suruh megang aji buat memotret fox\",\n",
    "            \"Gw bahkan ga berani ke depan pintu itu lagi, aji yang memotret fox di depan pintu akses bawah tanah, gw nunggu di luar, sehabis ini gw share fotonya\",\n",
    "            \"Foto di depan pintu akses bawah tanah. Lihat foto terakhir, mata itu bukan fox @Jaktvalken ! Si Aji @si_wel yang memotret sampai kaget sendiri.\",\n",
    "            \"sehabis foto2 di depan pintu akses bawah tanah, kami langsung memutuskan cabut dari lawang-sewu, fox kelihatan lemes banget gara2 kejadian itu\"]\n",
    "\n",
    "document2 = [\"Kalian ingat weekend kemarin saya ke Lawang-Sewu Semarang berbarengan @Jaktvalken @si_wel kan? #memetwit\",\n",
    "            \"Saya pagi sampai di Semarang, ketemuan berbarengan @Jaktvalken berbarengan @si_wel di simpang lima, kemudian sarapan gudeg. Setelah itu ke Lawang-Sewu bertiga\",\n",
    "            \"Sampai di Lawang-Sewu, saya sudah excited karena memang hobi memotret, setiap sudutnya tuh instagramable banget, dan saya tidak ada curiga apapun disitu\",\n",
    "            \"Saya berkeliling ke berbagai sudut ruangan, saya yang memang tidak terlalu peka merasa biasa, asyik memotret sana sini sambil sesekali bergaya bertiga\",\n",
    "            \"sampai ketika kami bertiga duduk persis di sebelah ini, fox @Jaktvalken mengatakan 'paman ada yang lihat kita dari tadi, di lantai 2'\",\n",
    "            \"saya mencoba biasa dan cuek, dan kami rencananya mau ke basement bekas penjara bawah tanah jaman dulu, tetapi sehabis istirahat dulu, lelah\",\n",
    "            \"Kami lanjut mencari jalan akses ke basement, tapi sayangnya wisata bawah tanah / basement lagi ditutup. Yasudah kan kami keliling lagi\",\n",
    "            \"Kami berkeliling ke lantai 2, lantai 3, semua dijelajah, pelampiasan karena ke basement ditutup. Ini siang hari, memang suasananya gitu\",\n",
    "            \"Ini foto-foto di lantai 3, saya rasa kamu bisa lihat sesuatu selain kita bertiga di foto ini, Entah lihat sendiri :|\",\n",
    "            \"@si_wel sehabis memotrer itu kita turun, sudah lelah juga, dan panas sangat, ketika mau selesai @Jaktvalken request 'paman, kita ke pintu akses basement lagi ayo'\",\n",
    "            \"Lagi-lagi saya masih tidak ada curiga apa-apa, kita kembali ke pintu akses ruang bawah tanah, kan memang saya yang bawa kamera, dan kemana-mana saya yang memotret\",\n",
    "            \"Tetapi tiba-tiba ketika memotret di depan sini, sepanjang ingatan saya. Saya baru memotret sekali atau dua kali ini, tiba-tiba saya mual ingin muntah, saya kabur ke toilet, lemas\",\n",
    "            \"Ada ketika saya hilang 10 menit, Aji berbarengan Fox mencari saya, sampai saya kembali ke situ sudah lemas, kamera saya suruh pegang aji untuk memotret fox\",\n",
    "            \"saya bahkan tidak berani ke depan pintu itu lagi, aji yang memotret fox di depan pintu akses bawah tanah, saya menunggu di luar, sehabis ini saya share fotonya\",\n",
    "            \"Foto di depan pintu akses bawah tanah. Lihat foto terakhir, mata itu bukan fox @Jaktvalken ! Si Aji @si_wel yang memotret sampai kaget sendiri.\",\n",
    "            \"sehabis memotret di depan pintu akses bawah tanah, kami langsung memutuskan pergi dari lawang-sewu, fox kelihatan lemas sangat gara-gara kejadian itu\"]\n",
    "\n",
    "document3 = [\"liburan keluarga di ancol\",\n",
    "             \"lagi nonton konser dengan keluarga\",\n",
    "             \"ancol ramai lagi konser\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document = document2\n",
    "len(document)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"saya sedang asik-asiknya makan\",\n",
    "\"lagi dengerin musik sambil minum air\",\n",
    "\"ketika disana, dia masih minum air\",\n",
    "\"aku mau pergi ke bandung\",\n",
    "\"habis ini aku langsung pergi ke luar\",\n",
    "\"waktu telah menunjukkan jam segini\",\n",
    "\"Ia ingin pergi mencari obat dengan berjalan\",\n",
    "\"Aku segera melakukan\",\n",
    "\"Tak disangka ternyata Andi membalasnya\",\n",
    "\"Pergi untuk kembali\",\n",
    "\"Toni masih mau memboncengkan Andi hingga sampai rumah\",\n",
    "\"Aku mau menyelesaikan tugaas\",\n",
    "\"Bel telah berbunyi sebagai tanda\",\n",
    "\"Perutnya terasa sangat lapar\",\n",
    "\"Budi lebih cerdas daripada Anton\",\n",
    "\"Anton menyuruh pembantunya agar lari\",\n",
    "\"Budi berpikir bahwa itu adalah bohong\",\n",
    "\"Jika nanti jadi pergi ke sana\",\n",
    "\"Dia terbayang kejadian yang dialami tadi pagi\",\n",
    "\"Hal itu sudah biasa dilakukan\",\n",
    "\"Dengan kekayaan itu\",\n",
    "\"Santo selalu membersihkan selokan\",\n",
    "\"Dengan penuh kesabaran\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "from preprocess.normalize import Normalize\n",
    "from preprocess.tokenize import Tokenize\n",
    "from preprocess.symspell import SymSpell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dictionary...\n",
      "Processing dictionary...\n",
      "Copied 94811 words to master dictionary...\n",
      "Copied 679534 hashes to master dictionary...\n"
     ]
    }
   ],
   "source": [
    "# initialize\n",
    "normalizer = Normalize()\n",
    "tokenizer = Tokenize()\n",
    "symspell = SymSpell(max_dictionary_edit_distance=3)\n",
    "symspell.load_complete_model_from_json('preprocess\\data\\corpus_complete_model.json', encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do process\n",
    "doc_preprocessed = []\n",
    "\n",
    "for tweet in document:\n",
    "    # normalize\n",
    "    tweet_norm = normalizer.remove_ascii_unicode(tweet)\n",
    "    tweet_norm = normalizer.remove_rt_fav(tweet_norm)\n",
    "    tweet_norm = normalizer.lower_text(tweet_norm)\n",
    "    tweet_norm = normalizer.remove_newline(tweet_norm)\n",
    "    tweet_norm = normalizer.remove_url(tweet_norm)\n",
    "    tweet_norm = normalizer.remove_emoticon(tweet_norm)\n",
    "    tweet_norm = normalizer.remove_hashtag_mention(tweet_norm)\n",
    "    tweet_norm = normalizer.remove_punctuation(tweet_norm)\n",
    "    \n",
    "    # tokenize\n",
    "    tweet_tok = tokenizer.WordTokenize(tweet_norm, removepunct=True)\n",
    "    \n",
    "    # spell correction\n",
    "    temp = []\n",
    "    for token in tweet_tok:\n",
    "        suggestion = symspell.lookup(phrase=token, verbosity=1, max_edit_distance=3)\n",
    "\n",
    "        # option if there is no suggestion\n",
    "        if len(suggestion) > 0:\n",
    "            get_suggestion = str(suggestion[0]).split(':')[0]\n",
    "            temp.append(get_suggestion)\n",
    "        else:\n",
    "            temp.append(token)\n",
    "    tweet_prepared = ' '.join(temp)\n",
    "    \n",
    "    doc_preprocessed.append(tweet_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kalian ingat weekend kemarin saya ke lawang-sewu semarang berbarengan kan',\n",
       " 'saya pagi sampai di semarang ketemuan berbarengan berbarengan di simpang lima kemudian sarapan gudeg setelah itu ke lawang-sewu bertiga',\n",
       " 'sampai di lawang-sewu saya sudah excited karena memang hobi memotret setiap mulutnya tuh instagramable banget dan saya tidak ada curiga apapun disitu',\n",
       " 'saya berkeliling ke berbagai sudut ruangan saya yang memang tidak terlalu peka merasa biasa asyik memotret sana sini sambil sesekali bergaya bertiga',\n",
       " 'sampai ketika kami bertiga duduk persis di sebelah ini fox mengatakan paman ada yang lihat kita dari tadi di lantai 2',\n",
       " 'saya mencoba biasa dan cuek dan kami rencananya mau ke basement bekas penjara bawah tanah jaman dulu tetapi sehabis istirahat dulu lelah',\n",
       " 'kami lanjut mencari jalan akses ke basement tapi sayangnya wisata bawah tanah basement lagi ditutup sudah kan kami keliling lagi',\n",
       " 'kami berkeliling ke lantai 2 lantai 3 semua dijelajah pelampiasan karena ke basement ditutup ini siang hari memang suasananya gitu',\n",
       " 'ini foto-foto di lantai 3 saya rasa kamu bisa lihat sesuatu selain kita bertiga di foto ini entah lihat sendiri',\n",
       " 'sehabis melotre itu kita turun sudah lelah juga dan panas sangat ketika mau selesai request paman kita ke pintu akses basement lagi ayo',\n",
       " 'lagi-lagi saya masih tidak ada curiga apa-apa kita kembali ke pintu akses ruang bawah tanah kan memang saya yang bawa kamera dan kemana-mana saya yang memotret',\n",
       " 'tetapi tiba-tiba ketika memotret di depan sini sepanjang ingatan saya saya baru memotret sekali atau dua kali ini tiba-tiba saya mual ingin muntah saya kabur ke toilet lemas',\n",
       " 'ada ketika saya hilang 10 menit aji berbarengan fox mencari saya sampai saya kembali ke situ sudah lemas kamera saya suruh pegang aji untuk memotret fox',\n",
       " 'saya bahkan tidak berani ke depan pintu itu lagi aji yang memotret fox di depan pintu akses bawah tanah saya menunggu di luar sehabis ini saya share fotonya',\n",
       " 'foto di depan pintu akses bawah tanah lihat foto terakhir mata itu bukan fox si aji yang memotret sampai kaget sendiri',\n",
       " 'sehabis memotret di depan pintu akses bawah tanah kami langsung memutuskan pergi dari lawang-sewu fox kelihatan lemas sangat gara-gara kejadian itu']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_preprocessed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POSLDA\n",
    "## HMM Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "from hmmtagger.tagger import MainTagger\n",
    "from tokenization import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize\n",
    "tagger = MainTagger(\"resource/Lexicon.trn\", \"resource/Ngram.trn\", 0, 3, 3, 0, 0, False, 0.2, 0, 500.0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do process\n",
    "doc_tagged = []\n",
    "\n",
    "for tweet in doc_preprocessed:\n",
    "    if len(tweet) == 0: continue\n",
    "    out = sentence_extraction(cleaning(tweet))\n",
    "\n",
    "    join_token = []\n",
    "    for o in out:\n",
    "        strtag = \" \".join(tokenisasi_kalimat(o)).strip()\n",
    "        join_token.extend(tagger.taggingStr(strtag))\n",
    "    doc_tagged.append(' '.join(join_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kalian/NN ingat/VBT weekend/NN kemarin/NN saya/PRP ke/IN lawang-sewu/NN semarang/NN berbarengan/VBI kan/RP\n",
      "saya/PRP pagi/NN sampai/VBT di/IN semarang/NN ketemuan/NN berbarengan/VBI berbarengan/VBI di/IN simpang/NN lima/CDP kemudian/SC sarapan/NN gudeg/NN setelah/SC itu/DT ke/IN lawang-sewu/NN bertiga/CDI\n",
      "sampai/VBT di/IN lawang-sewu/NN saya/PRP sudah/MD excited/NN karena/CC memang/RB hobi/NN memotret/VBT setiap/DT mulutnya/NN tuh/NN instagramable/NN banget/NN dan/CC saya/PRP tidak/NEG ada/VBI curiga/NN apapun/NN disitu/VBT\n",
      "saya/PRP berkeliling/VBI ke/IN berbagai/CDI sudut/NNP ruangan/NN saya/PRP yang/SC memang/RB tidak/NEG terlalu/RB peka/NN merasa/VBT biasa/JJ asyik/JJ memotret/VBT sana/PRL sini/PRL sambil/SC sesekali/RB bergaya/VBT bertiga/CDI\n",
      "sampai/VBT ketika/SC kami/PRP bertiga/CDI duduk/VBI persis/NN di/IN sebelah/NN ini/DT fox/NN mengatakan/VBI paman/NN ada/VBI yang/SC lihat/VBT kita/PRP dari/IN tadi/RB di/IN lantai/NN 2/CDP\n",
      "saya/PRP mencoba/VBT biasa/JJ dan/CC cuek/JJ dan/CC kami/PRP rencananya/NN mau/CC ke/IN basement/NN bekas/VBI penjara/NN bawah/NN tanah/NN jaman/NN dulu/RB tetapi/CC sehabis/SC istirahat/NN dulu/RB lelah/JJ\n",
      "kami/PRP lanjut/VBT mencari/VBT jalan/NN akses/NN ke/IN basement/NN tapi/CC sayangnya/NN wisata/FW bawah/NN tanah/NN basement/NN lagi/RB ditutup/VBT sudah/MD kan/RP kami/PRP keliling/NN lagi/RB\n",
      "kami/PRP berkeliling/VBI ke/IN lantai/NN 2/CDP lantai/NN 3/CDP semua/NN dijelajah/VBT pelampiasan/NN karena/CC ke/IN basement/NN ditutup/VBT ini/DT siang/NN hari/NN memang/RB suasananya/NN gitu/NN\n",
      "ini/DT foto-foto/NN di/IN lantai/NN 3/CDP saya/PRP rasa/NN kamu/PRP bisa/MD lihat/VBT sesuatu/NN selain/SC kita/PRP bertiga/CDI di/IN foto/NN ini/DT entah/RB lihat/VBT sendiri/JJ\n",
      "sehabis/SC melotre/VBT itu/DT kita/PRP turun/VBI sudah/RB lelah/JJ juga/RB dan/CC panas/JJ sangat/RB ketika/SC mau/CC selesai/NN request/NN paman/NN kita/PRP ke/IN pintu/NN akses/NN basement/NN lagi/RB ayo/NN\n",
      "lagi-lagi/RB saya/PRP masih/RB tidak/NEG ada/VBI curiga/NN apa-apa/NN kita/PRP kembali/RB ke/IN pintu/NN akses/NN ruang/NN bawah/NN tanah/NN kan/RP memang/RB saya/PRP yang/SC bawa/NN kamera/NN dan/CC kemana-mana/NN saya/PRP yang/SC memotret/VBT\n",
      "tetapi/CC tiba-tiba/NN ketika/SC memotret/VBT di/IN depan/NN sini/PRL sepanjang/RB ingatan/NNP saya/PRP saya/PRP baru/JJ memotret/VBT sekali/RB atau/CC dua/CDP kali/NN ini/DT tiba-tiba/NN saya/PRP mual/JJ ingin/VBT muntah/VBT saya/PRP kabur/NN ke/IN toilet/NN lemas/JJ\n",
      "ada/VBI ketika/SC saya/PRP hilang/VBI 10/CDP menit/NN aji/NN berbarengan/VBT fox/NN mencari/VBT saya/PRP sampai/VBT saya/PRP kembali/RB ke/IN situ/NN sudah/MD lemas/JJ kamera/NN saya/PRP suruh/NN pegang/VBI aji/NN untuk/IN memotret/VBT fox/NN\n",
      "saya/PRP bahkan/RB tidak/NEG berani/JJ ke/IN depan/NN pintu/NN itu/DT lagi/RB aji/NN yang/SC memotret/VBT fox/NN di/IN depan/NN pintu/NN akses/NN bawah/NN tanah/NN saya/PRP menunggu/VBI di/IN luar/JJ sehabis/SC ini/DT saya/PRP share/NN fotonya/NN\n",
      "foto/NN di/IN depan/NN pintu/NN akses/NN bawah/NN tanah/NN lihat/VBT foto/NN terakhir/JJ mata/NN itu/DT bukan/NEG fox/NN si/RP aji/NN yang/SC memotret/VBT sampai/VBT kaget/NNP sendiri/JJ\n",
      "sehabis/SC memotret/VBT di/IN depan/NN pintu/NN akses/NN bawah/NN tanah/NN kami/PRP langsung/JJ memutuskan/VBI pergi/VBI dari/IN lawang-sewu/NN fox/NN kelihatan/VBI lemas/JJ sangat/RB gara-gara/NN kejadian/NN itu/DT\n"
     ]
    }
   ],
   "source": [
    "for _ in doc_tagged:\n",
    "    print(_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kelas Tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define\n",
    "Ccon = ['JJ', 'NN','NNP', 'NNG', 'VBI', 'VBT']\n",
    "Cfnc = ['OP', 'CP', 'GM', ';', ':', '\"', '.',\n",
    "         ',', '-', '...', 'RB', 'IN', 'MD', 'CC',\n",
    "         'SC', 'DT', 'UH', 'CDO', 'CDC', 'CDP', 'CDI',\n",
    "         'PRP', 'WP', 'PRN', 'PRL', 'NEG', 'SYM', 'RP', 'FW']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do process\n",
    "doc_classified = []\n",
    "\n",
    "for tweet in doc_tagged:\n",
    "    tweet_split = tweet.split(' ')\n",
    "    \n",
    "    temp = {\"Content\": [], \"Function\": []}\n",
    "    con = []\n",
    "    fnc = []\n",
    "    \n",
    "    for token in tweet_split:\n",
    "        word = token.split('/', 1)[0]\n",
    "        tag = token.split('/', 1)[1]\n",
    "        \n",
    "        if tag in Ccon:\n",
    "            con.append(token)\n",
    "        elif tag in Cfnc:\n",
    "            fnc.append(token)\n",
    "            \n",
    "    temp[\"Content\"].append(' '.join(con))\n",
    "    temp[\"Function\"].append(' '.join(fnc))\n",
    "    \n",
    "    doc_classified.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Content': ['kalian/NN ingat/VBT weekend/NN kemarin/NN lawang-sewu/NN semarang/NN berbarengan/VBI'],\n",
       "  'Function': ['saya/PRP ke/IN kan/RP']},\n",
       " {'Content': ['pagi/NN sampai/VBT semarang/NN ketemuan/NN berbarengan/VBI berbarengan/VBI simpang/NN sarapan/NN gudeg/NN lawang-sewu/NN'],\n",
       "  'Function': ['saya/PRP di/IN di/IN lima/CDP kemudian/SC setelah/SC itu/DT ke/IN bertiga/CDI']},\n",
       " {'Content': ['sampai/VBT lawang-sewu/NN excited/NN hobi/NN memotret/VBT mulutnya/NN tuh/NN instagramable/NN banget/NN ada/VBI curiga/NN apapun/NN disitu/VBT'],\n",
       "  'Function': ['di/IN saya/PRP sudah/MD karena/CC memang/RB setiap/DT dan/CC saya/PRP tidak/NEG']},\n",
       " {'Content': ['berkeliling/VBI sudut/NNP ruangan/NN peka/NN merasa/VBT biasa/JJ asyik/JJ memotret/VBT bergaya/VBT'],\n",
       "  'Function': ['saya/PRP ke/IN berbagai/CDI saya/PRP yang/SC memang/RB tidak/NEG terlalu/RB sana/PRL sini/PRL sambil/SC sesekali/RB bertiga/CDI']},\n",
       " {'Content': ['sampai/VBT duduk/VBI persis/NN sebelah/NN fox/NN mengatakan/VBI paman/NN ada/VBI lihat/VBT lantai/NN'],\n",
       "  'Function': ['ketika/SC kami/PRP bertiga/CDI di/IN ini/DT yang/SC kita/PRP dari/IN tadi/RB di/IN 2/CDP']},\n",
       " {'Content': ['mencoba/VBT biasa/JJ cuek/JJ rencananya/NN basement/NN bekas/VBI penjara/NN bawah/NN tanah/NN jaman/NN istirahat/NN lelah/JJ'],\n",
       "  'Function': ['saya/PRP dan/CC dan/CC kami/PRP mau/CC ke/IN dulu/RB tetapi/CC sehabis/SC dulu/RB']},\n",
       " {'Content': ['lanjut/VBT mencari/VBT jalan/NN akses/NN basement/NN sayangnya/NN bawah/NN tanah/NN basement/NN ditutup/VBT keliling/NN'],\n",
       "  'Function': ['kami/PRP ke/IN tapi/CC wisata/FW lagi/RB sudah/MD kan/RP kami/PRP lagi/RB']},\n",
       " {'Content': ['berkeliling/VBI lantai/NN lantai/NN semua/NN dijelajah/VBT pelampiasan/NN basement/NN ditutup/VBT siang/NN hari/NN suasananya/NN gitu/NN'],\n",
       "  'Function': ['kami/PRP ke/IN 2/CDP 3/CDP karena/CC ke/IN ini/DT memang/RB']},\n",
       " {'Content': ['foto-foto/NN lantai/NN rasa/NN lihat/VBT sesuatu/NN foto/NN lihat/VBT sendiri/JJ'],\n",
       "  'Function': ['ini/DT di/IN 3/CDP saya/PRP kamu/PRP bisa/MD selain/SC kita/PRP bertiga/CDI di/IN ini/DT entah/RB']},\n",
       " {'Content': ['melotre/VBT turun/VBI lelah/JJ panas/JJ selesai/NN request/NN paman/NN pintu/NN akses/NN basement/NN ayo/NN'],\n",
       "  'Function': ['sehabis/SC itu/DT kita/PRP sudah/RB juga/RB dan/CC sangat/RB ketika/SC mau/CC kita/PRP ke/IN lagi/RB']},\n",
       " {'Content': ['ada/VBI curiga/NN apa-apa/NN pintu/NN akses/NN ruang/NN bawah/NN tanah/NN bawa/NN kamera/NN kemana-mana/NN memotret/VBT'],\n",
       "  'Function': ['lagi-lagi/RB saya/PRP masih/RB tidak/NEG kita/PRP kembali/RB ke/IN kan/RP memang/RB saya/PRP yang/SC dan/CC saya/PRP yang/SC']},\n",
       " {'Content': ['tiba-tiba/NN memotret/VBT depan/NN ingatan/NNP baru/JJ memotret/VBT kali/NN tiba-tiba/NN mual/JJ ingin/VBT muntah/VBT kabur/NN toilet/NN lemas/JJ'],\n",
       "  'Function': ['tetapi/CC ketika/SC di/IN sini/PRL sepanjang/RB saya/PRP saya/PRP sekali/RB atau/CC dua/CDP ini/DT saya/PRP saya/PRP ke/IN']},\n",
       " {'Content': ['ada/VBI hilang/VBI menit/NN aji/NN berbarengan/VBT fox/NN mencari/VBT sampai/VBT situ/NN lemas/JJ kamera/NN suruh/NN pegang/VBI aji/NN memotret/VBT fox/NN'],\n",
       "  'Function': ['ketika/SC saya/PRP 10/CDP saya/PRP saya/PRP kembali/RB ke/IN sudah/MD saya/PRP untuk/IN']},\n",
       " {'Content': ['berani/JJ depan/NN pintu/NN aji/NN memotret/VBT fox/NN depan/NN pintu/NN akses/NN bawah/NN tanah/NN menunggu/VBI luar/JJ share/NN fotonya/NN'],\n",
       "  'Function': ['saya/PRP bahkan/RB tidak/NEG ke/IN itu/DT lagi/RB yang/SC di/IN saya/PRP di/IN sehabis/SC ini/DT saya/PRP']},\n",
       " {'Content': ['foto/NN depan/NN pintu/NN akses/NN bawah/NN tanah/NN lihat/VBT foto/NN terakhir/JJ mata/NN fox/NN aji/NN memotret/VBT sampai/VBT kaget/NNP sendiri/JJ'],\n",
       "  'Function': ['di/IN itu/DT bukan/NEG si/RP yang/SC']},\n",
       " {'Content': ['memotret/VBT depan/NN pintu/NN akses/NN bawah/NN tanah/NN langsung/JJ memutuskan/VBI pergi/VBI lawang-sewu/NN fox/NN kelihatan/VBI lemas/JJ gara-gara/NN kejadian/NN'],\n",
       "  'Function': ['sehabis/SC di/IN kami/PRP dari/IN sangat/RB itu/DT']}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kalian/NN ingat/VBT weekend/NN kemarin/NN lawang-sewu/NN semarang/NN berbarengan/VBI', 'pagi/NN sampai/VBT semarang/NN ketemuan/NN berbarengan/VBI berbarengan/VBI simpang/NN sarapan/NN gudeg/NN lawang-sewu/NN', 'sampai/VBT lawang-sewu/NN excited/NN hobi/NN memotret/VBT mulutnya/NN tuh/NN instagramable/NN banget/NN ada/VBI curiga/NN apapun/NN disitu/VBT', 'berkeliling/VBI sudut/NNP ruangan/NN peka/NN merasa/VBT biasa/JJ asyik/JJ memotret/VBT bergaya/VBT', 'sampai/VBT duduk/VBI persis/NN sebelah/NN fox/NN mengatakan/VBI paman/NN ada/VBI lihat/VBT lantai/NN', 'mencoba/VBT biasa/JJ cuek/JJ rencananya/NN basement/NN bekas/VBI penjara/NN bawah/NN tanah/NN jaman/NN istirahat/NN lelah/JJ', 'lanjut/VBT mencari/VBT jalan/NN akses/NN basement/NN sayangnya/NN bawah/NN tanah/NN basement/NN ditutup/VBT keliling/NN', 'berkeliling/VBI lantai/NN lantai/NN semua/NN dijelajah/VBT pelampiasan/NN basement/NN ditutup/VBT siang/NN hari/NN suasananya/NN gitu/NN', 'foto-foto/NN lantai/NN rasa/NN lihat/VBT sesuatu/NN foto/NN lihat/VBT sendiri/JJ', 'melotre/VBT turun/VBI lelah/JJ panas/JJ selesai/NN request/NN paman/NN pintu/NN akses/NN basement/NN ayo/NN', 'ada/VBI curiga/NN apa-apa/NN pintu/NN akses/NN ruang/NN bawah/NN tanah/NN bawa/NN kamera/NN kemana-mana/NN memotret/VBT', 'tiba-tiba/NN memotret/VBT depan/NN ingatan/NNP baru/JJ memotret/VBT kali/NN tiba-tiba/NN mual/JJ ingin/VBT muntah/VBT kabur/NN toilet/NN lemas/JJ', 'ada/VBI hilang/VBI menit/NN aji/NN berbarengan/VBT fox/NN mencari/VBT sampai/VBT situ/NN lemas/JJ kamera/NN suruh/NN pegang/VBI aji/NN memotret/VBT fox/NN', 'berani/JJ depan/NN pintu/NN aji/NN memotret/VBT fox/NN depan/NN pintu/NN akses/NN bawah/NN tanah/NN menunggu/VBI luar/JJ share/NN fotonya/NN', 'foto/NN depan/NN pintu/NN akses/NN bawah/NN tanah/NN lihat/VBT foto/NN terakhir/JJ mata/NN fox/NN aji/NN memotret/VBT sampai/VBT kaget/NNP sendiri/JJ', 'memotret/VBT depan/NN pintu/NN akses/NN bawah/NN tanah/NN langsung/JJ memutuskan/VBI pergi/VBI lawang-sewu/NN fox/NN kelihatan/VBI lemas/JJ gara-gara/NN kejadian/NN']\n",
      "\n",
      "[['kalian', 'ingat', 'weekend', 'kemarin', 'lawang-sewu', 'semarang', 'berbarengan'], ['pagi', 'sampai', 'semarang', 'ketemuan', 'berbarengan', 'berbarengan', 'simpang', 'sarapan', 'gudeg', 'lawang-sewu'], ['sampai', 'lawang-sewu', 'excited', 'hobi', 'memotret', 'mulutnya', 'tuh', 'instagramable', 'banget', 'ada', 'curiga', 'apapun', 'disitu'], ['berkeliling', 'sudut', 'ruangan', 'peka', 'merasa', 'biasa', 'asyik', 'memotret', 'bergaya'], ['sampai', 'duduk', 'persis', 'sebelah', 'fox', 'mengatakan', 'paman', 'ada', 'lihat', 'lantai'], ['mencoba', 'biasa', 'cuek', 'rencananya', 'basement', 'bekas', 'penjara', 'bawah', 'tanah', 'jaman', 'istirahat', 'lelah'], ['lanjut', 'mencari', 'jalan', 'akses', 'basement', 'sayangnya', 'bawah', 'tanah', 'basement', 'ditutup', 'keliling'], ['berkeliling', 'lantai', 'lantai', 'semua', 'dijelajah', 'pelampiasan', 'basement', 'ditutup', 'siang', 'hari', 'suasananya', 'gitu'], ['foto-foto', 'lantai', 'rasa', 'lihat', 'sesuatu', 'foto', 'lihat', 'sendiri'], ['melotre', 'turun', 'lelah', 'panas', 'selesai', 'request', 'paman', 'pintu', 'akses', 'basement', 'ayo'], ['ada', 'curiga', 'apa-apa', 'pintu', 'akses', 'ruang', 'bawah', 'tanah', 'bawa', 'kamera', 'kemana-mana', 'memotret'], ['tiba-tiba', 'memotret', 'depan', 'ingatan', 'baru', 'memotret', 'kali', 'tiba-tiba', 'mual', 'ingin', 'muntah', 'kabur', 'toilet', 'lemas'], ['ada', 'hilang', 'menit', 'aji', 'berbarengan', 'fox', 'mencari', 'sampai', 'situ', 'lemas', 'kamera', 'suruh', 'pegang', 'aji', 'memotret', 'fox'], ['berani', 'depan', 'pintu', 'aji', 'memotret', 'fox', 'depan', 'pintu', 'akses', 'bawah', 'tanah', 'menunggu', 'luar', 'share', 'fotonya'], ['foto', 'depan', 'pintu', 'akses', 'bawah', 'tanah', 'lihat', 'foto', 'terakhir', 'mata', 'fox', 'aji', 'memotret', 'sampai', 'kaget', 'sendiri'], ['memotret', 'depan', 'pintu', 'akses', 'bawah', 'tanah', 'langsung', 'memutuskan', 'pergi', 'lawang-sewu', 'fox', 'kelihatan', 'lemas', 'gara-gara', 'kejadian']]\n"
     ]
    }
   ],
   "source": [
    "# split document content and function\n",
    "doc_content = []\n",
    "for tweet in doc_classified:\n",
    "    doc_content.append(''.join(tweet['Content']))\n",
    "\n",
    "print(doc_content)\n",
    "print()\n",
    "\n",
    "# split tag and word\n",
    "doc_prepared = []\n",
    "for tweet in doc_content:\n",
    "    tweet_split = tweet.split(' ')\n",
    "    \n",
    "    temp = []\n",
    "    for token in tweet_split:\n",
    "        word = token.split('/', 1)[0]\n",
    "        temp.append(word)\n",
    "    \n",
    "    doc_prepared.append(temp)\n",
    "\n",
    "print(doc_prepared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "from lda.ldamodel import LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# iniatialize\n",
    "k = 2\n",
    "alpha = 0.01\n",
    "beta = 0.01\n",
    "iterations = 1000\n",
    "\n",
    "# do process\n",
    "lda = LdaModel(doc_prepared, k, alpha, beta, iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = lda.get_topic_word_pwz(doc_content)\n",
    "\n",
    "df_lda = pd.DataFrame(result, columns=['Topik', 'Kata', 'PWZ'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topik</th>\n",
       "      <th>Kata</th>\n",
       "      <th>PWZ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>memotret/VBT</td>\n",
       "      <td>0.058353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>akses/NN</td>\n",
       "      <td>0.050029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>pintu/NN</td>\n",
       "      <td>0.050029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>fox/NN</td>\n",
       "      <td>0.050029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>sampai/VBT</td>\n",
       "      <td>0.041705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>depan/NN</td>\n",
       "      <td>0.041705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>berbarengan/VBI</td>\n",
       "      <td>0.033381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>berbarengan/VBT</td>\n",
       "      <td>0.033381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>lawang-sewu/NN</td>\n",
       "      <td>0.033381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>bawah/NN</td>\n",
       "      <td>0.033381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>ada/VBI</td>\n",
       "      <td>0.033381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>aji/NN</td>\n",
       "      <td>0.033381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>tanah/NN</td>\n",
       "      <td>0.025056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>lemas/JJ</td>\n",
       "      <td>0.025056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>semarang/NN</td>\n",
       "      <td>0.016732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>mencari/VBT</td>\n",
       "      <td>0.016732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>curiga/NN</td>\n",
       "      <td>0.016732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>kamera/NN</td>\n",
       "      <td>0.016732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>tiba-tiba/NN</td>\n",
       "      <td>0.016732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>weekend/NN</td>\n",
       "      <td>0.008408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>basement/NN</td>\n",
       "      <td>0.068508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>lihat/VBT</td>\n",
       "      <td>0.054834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>lantai/NN</td>\n",
       "      <td>0.054834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>tanah/NN</td>\n",
       "      <td>0.041160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>foto/NN</td>\n",
       "      <td>0.041160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>biasa/JJ</td>\n",
       "      <td>0.027485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>memotret/VBT</td>\n",
       "      <td>0.027485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>lelah/JJ</td>\n",
       "      <td>0.027485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>ditutup/VBT</td>\n",
       "      <td>0.027485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>bawah/NN</td>\n",
       "      <td>0.027485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1</td>\n",
       "      <td>berkeliling/VBI</td>\n",
       "      <td>0.027485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1</td>\n",
       "      <td>paman/NN</td>\n",
       "      <td>0.027485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1</td>\n",
       "      <td>sendiri/JJ</td>\n",
       "      <td>0.027485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1</td>\n",
       "      <td>ruangan/NN</td>\n",
       "      <td>0.013811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1</td>\n",
       "      <td>merasa/VBT</td>\n",
       "      <td>0.013811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1</td>\n",
       "      <td>mencoba/VBT</td>\n",
       "      <td>0.013811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1</td>\n",
       "      <td>rencananya/NN</td>\n",
       "      <td>0.013811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1</td>\n",
       "      <td>bekas/VBI</td>\n",
       "      <td>0.013811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1</td>\n",
       "      <td>penjara/NN</td>\n",
       "      <td>0.013811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1</td>\n",
       "      <td>jaman/NN</td>\n",
       "      <td>0.013811</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topik             Kata       PWZ\n",
       "0       0     memotret/VBT  0.058353\n",
       "1       0         akses/NN  0.050029\n",
       "2       0         pintu/NN  0.050029\n",
       "3       0           fox/NN  0.050029\n",
       "4       0       sampai/VBT  0.041705\n",
       "5       0         depan/NN  0.041705\n",
       "6       0  berbarengan/VBI  0.033381\n",
       "7       0  berbarengan/VBT  0.033381\n",
       "8       0   lawang-sewu/NN  0.033381\n",
       "9       0         bawah/NN  0.033381\n",
       "10      0          ada/VBI  0.033381\n",
       "11      0           aji/NN  0.033381\n",
       "12      0         tanah/NN  0.025056\n",
       "13      0         lemas/JJ  0.025056\n",
       "14      0      semarang/NN  0.016732\n",
       "15      0      mencari/VBT  0.016732\n",
       "16      0        curiga/NN  0.016732\n",
       "17      0        kamera/NN  0.016732\n",
       "18      0     tiba-tiba/NN  0.016732\n",
       "19      0       weekend/NN  0.008408\n",
       "20      1      basement/NN  0.068508\n",
       "21      1        lihat/VBT  0.054834\n",
       "22      1        lantai/NN  0.054834\n",
       "23      1         tanah/NN  0.041160\n",
       "24      1          foto/NN  0.041160\n",
       "25      1         biasa/JJ  0.027485\n",
       "26      1     memotret/VBT  0.027485\n",
       "27      1         lelah/JJ  0.027485\n",
       "28      1      ditutup/VBT  0.027485\n",
       "29      1         bawah/NN  0.027485\n",
       "30      1  berkeliling/VBI  0.027485\n",
       "31      1         paman/NN  0.027485\n",
       "32      1       sendiri/JJ  0.027485\n",
       "33      1       ruangan/NN  0.013811\n",
       "34      1       merasa/VBT  0.013811\n",
       "35      1      mencoba/VBT  0.013811\n",
       "36      1    rencananya/NN  0.013811\n",
       "37      1        bekas/VBI  0.013811\n",
       "38      1       penjara/NN  0.013811\n",
       "39      1         jaman/NN  0.013811"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.058353\n",
      "0.050029\n",
      "0.050029\n",
      "0.050029\n",
      "0.041705\n",
      "0.041705\n",
      "0.033381\n",
      "0.033381\n",
      "0.033381\n",
      "0.033381\n",
      "0.033381\n",
      "0.033381\n",
      "0.025056\n",
      "0.025056\n",
      "0.016732\n",
      "0.016732\n",
      "0.016732\n",
      "0.016732\n",
      "0.016732\n",
      "0.008408\n"
     ]
    }
   ],
   "source": [
    "print(df_lda[df_lda['Topik']==0]['PWZ'].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.678557377440629"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.perplexity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "dict_ldapwz = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in df_lda.iterrows():\n",
    "#     dict_ldapwz[row['Topik']].append(row['Kata'])\n",
    "    dict_ldapwz[row['Topik']].append([row['Kata'], row['PWZ']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {0: [['memotret/VBT', 0.05835345042870224],\n",
       "              ['akses/NN', 0.050029135103637726],\n",
       "              ['pintu/NN', 0.050029135103637726],\n",
       "              ['fox/NN', 0.050029135103637726],\n",
       "              ['sampai/VBT', 0.04170481977857321],\n",
       "              ['depan/NN', 0.04170481977857321],\n",
       "              ['berbarengan/VBI', 0.0333805044535087],\n",
       "              ['berbarengan/VBT', 0.0333805044535087],\n",
       "              ['lawang-sewu/NN', 0.0333805044535087],\n",
       "              ['bawah/NN', 0.0333805044535087],\n",
       "              ['ada/VBI', 0.0333805044535087],\n",
       "              ['aji/NN', 0.0333805044535087],\n",
       "              ['tanah/NN', 0.025056189128444185],\n",
       "              ['lemas/JJ', 0.025056189128444185],\n",
       "              ['semarang/NN', 0.01673187380337967],\n",
       "              ['mencari/VBT', 0.01673187380337967],\n",
       "              ['curiga/NN', 0.01673187380337967],\n",
       "              ['kamera/NN', 0.01673187380337967],\n",
       "              ['tiba-tiba/NN', 0.01673187380337967],\n",
       "              ['weekend/NN', 0.008407558478315159]],\n",
       "             1: [['basement/NN', 0.06850813619581567],\n",
       "              ['lihat/VBT', 0.054833857514016135],\n",
       "              ['lantai/NN', 0.054833857514016135],\n",
       "              ['tanah/NN', 0.0411595788322166],\n",
       "              ['foto/NN', 0.0411595788322166],\n",
       "              ['biasa/JJ', 0.027485300150417065],\n",
       "              ['memotret/VBT', 0.027485300150417065],\n",
       "              ['lelah/JJ', 0.027485300150417065],\n",
       "              ['ditutup/VBT', 0.027485300150417065],\n",
       "              ['bawah/NN', 0.027485300150417065],\n",
       "              ['berkeliling/VBI', 0.027485300150417065],\n",
       "              ['paman/NN', 0.027485300150417065],\n",
       "              ['sendiri/JJ', 0.027485300150417065],\n",
       "              ['ruangan/NN', 0.013811021468617532],\n",
       "              ['merasa/VBT', 0.013811021468617532],\n",
       "              ['mencoba/VBT', 0.013811021468617532],\n",
       "              ['rencananya/NN', 0.013811021468617532],\n",
       "              ['bekas/VBI', 0.013811021468617532],\n",
       "              ['penjara/NN', 0.013811021468617532],\n",
       "              ['jaman/NN', 0.013811021468617532]]})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_ldapwz"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "start_grammar = \"\"\"\n",
    "    S -> NP VP\n",
    "    NP -> NN | NN NN | NN JJ | NNG | NNP |  NP PP |\n",
    "    VP -> VBT NN | VBT NN NN | VBT NN CC NN | VBT NP | VBI | JJ\n",
    "    PP -> IN NP\n",
    "    IN -> 'di'\n",
    "    VBT -> 'memiliki', 'adalah', 'merupakan', 'terdapat', 'yaitu', 'sebagai', 'mempunyai'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import OrderedDict, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "NP = ['_NN', '_NNG', '_NNP']\n",
    "VP = ['_VBT _NN', '_MD _VBT _NN', '_VBT _NN _DT', '_VBT _NN _SC _JJ', '_VBT _NN _JJ', '_RB _VBT _NP', '_VBT _RB _JJ', '_VBT _NN _NN', '_SC _VBT _NP', '_VBI', '_JJ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_tag(dict_words_by_tag):\n",
    "    result = []\n",
    "    for key in dict_words_by_tag:\n",
    "        result.append(key)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_base_grammar(list_tag):\n",
    "    result = {}\n",
    "    \n",
    "    if '_VBT' not in list_tag:\n",
    "        list_tag.append('_VBT')\n",
    "    \n",
    "    if '_VBI' not in list_tag:\n",
    "        list_tag.append('_VBI')\n",
    "#     print(list_tag)\n",
    "    S = {\"_S\": [\"_NP _VP\"]}\n",
    "    PP = {\"_PP\": [\"_IN _NP\"]}\n",
    "    \n",
    "    if '_JJ' in list_tag:\n",
    "        NP_RULES = generate_NP(list_tag)\n",
    "        NP = {\"_NP\": NP_RULES}\n",
    "        if check_VP(list_tag):\n",
    "            VP_RULES = ['_JJ'] + generate_VP(list_tag)\n",
    "            VP = {\"_VP\": VP_RULES}\n",
    "        else:\n",
    "            VP = {\"_VP\": ['_JJ', '_PP']}\n",
    "\n",
    "        for r in [S, NP, VP, PP]:\n",
    "            result.update(r)\n",
    "        return result\n",
    "    else:\n",
    "        NP_RULES = remove_JJ(generate_NP(list_tag))\n",
    "        NP = {\"_NP\": NP_RULES}\n",
    "        if check_VP(list_tag):\n",
    "            VP_RULES = remove_JJ(generate_VP(list_tag))\n",
    "            VP = {\"_VP\": VP_RULES}\n",
    "        else:\n",
    "            VP = {\"_VP\": ['_PP']}\n",
    "            \n",
    "        for r in [S, NP, VP, PP]:\n",
    "            result.update(r)\n",
    "        return result\n",
    "\n",
    "def generate_NP(list_tag):\n",
    "    result = []\n",
    "    for tag in list_tag:\n",
    "        for words in NP:\n",
    "            if re.search(r'\\b' + tag + r'\\b', words):\n",
    "                result.append(words)\n",
    "    return list(OrderedDict.fromkeys(result))\n",
    "    \n",
    "def check_VP(list_tag):\n",
    "    for tag in list_tag:\n",
    "        if 'V' in tag:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def generate_VP(list_tag):\n",
    "    result = []\n",
    "    for tag in list_tag:\n",
    "        for words in VP:\n",
    "            if re.search(r'\\b' + tag + r'\\b', words):\n",
    "                result.append(words)\n",
    "    return list(OrderedDict.fromkeys(result))\n",
    "\n",
    "def remove_JJ(list_tag):\n",
    "    result = []\n",
    "    for tag in list_tag:\n",
    "        if '_JJ' in tag:\n",
    "            continue\n",
    "        else:\n",
    "            result.append(tag)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_words_grammar(dict_words_by_tag):\n",
    "    result = {}\n",
    "    IN = {\"_IN\": ['di', 'dengan', 'untuk']}\n",
    "    CC = {\"_CC\": ['dan', 'mau']}\n",
    "    SC = {\"_SC\": ['yang']}\n",
    "    RB = {\"_RB\": ['sedang', 'sambil', 'masih', 'lagi']}\n",
    "    DT = {\"_DT\": ['itu']}\n",
    "    MD = {\"_MD\": ['bisa', 'telah', 'sudah']}\n",
    "    ADD_VBT = {\"_VBT\": ['adalah', 'ingin', 'pengin']}\n",
    "    ADD_VBI = {\"_VBI\": ['ada', 'suka']}\n",
    "    \n",
    "    WORDS = dict_words_by_tag\n",
    "    if '_VBT' in WORDS:\n",
    "        for word in ADD_VBT[\"_VBT\"]:\n",
    "            if word not in WORDS['_VBT']:\n",
    "                WORDS['_VBT'].append(word)\n",
    "    else:\n",
    "        WORDS.update(ADD_VBT)\n",
    "    \n",
    "    if '_VBI' in WORDS:\n",
    "        for word in ADD_VBI[\"_VBI\"]:\n",
    "            if word not in WORDS['_VBI']:\n",
    "                WORDS['_VBI'].append(word)\n",
    "    else:\n",
    "        WORDS.update(ADD_VBI)  \n",
    "            \n",
    "    for r in [IN, CC, SC, RB, DT, MD, WORDS]:\n",
    "        result.update(r)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def organize_words_by_tag(list_words):\n",
    "    result = defaultdict(list)\n",
    "    \n",
    "    i = []\n",
    "    for s in list_words:\n",
    "        word, pwz = s[0], s[1]\n",
    "        \n",
    "        wrd = word.split('/')[0]\n",
    "        tag = word.split('/')[1]\n",
    "        result['_'+tag].append([wrd, pwz])\n",
    "    return dict(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_word_pwz(dict_words_pwz_by_tag):\n",
    "    dict_word_by_tag = defaultdict(list)\n",
    "    dict_pwz_by_tag = defaultdict(list)\n",
    "    \n",
    "    for key, values in dict_words_pwz_by_tag.items():\n",
    "        for data in values:\n",
    "            word, pwz = data[0], data[1]\n",
    "            dict_word_by_tag[key].append(word)\n",
    "            dict_pwz_by_tag[key].append(pwz)\n",
    "    return dict(dict_word_by_tag), dict(dict_pwz_by_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_grammar(dict_words_by_topic):\n",
    "    grammar = {}\n",
    "    \n",
    "    dict_words_pwz_by_tag = organize_words_by_tag(dict_words_by_topic)\n",
    "    list_tag = get_list_tag(dict_words_pwz_by_tag)\n",
    "    base_grammar = generate_base_grammar(list_tag)\n",
    "    dict_word_by_tag, dict_pwz_by_tag = split_word_pwz(dict_words_pwz_by_tag)\n",
    "    words_grammar = generate_words_grammar(dict_word_by_tag)\n",
    "    \n",
    "    for r in [base_grammar, words_grammar]:\n",
    "        grammar.update(r)\n",
    "    return grammar, dict_pwz_by_tag "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_two_dicts(x, y):\n",
    "    z = x.copy()\n",
    "    z.update(y)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_terminal(token):\n",
    "    return token[0] != \"_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_random = random.SystemRandom()\n",
    "\n",
    "def expand(grammar, tokens, dict_pwz_by_tag):\n",
    "#     print(tokens)\n",
    "    for i, token in enumerate(tokens):\n",
    "\n",
    "        # skip over terminals\n",
    "        if is_terminal(token): continue\n",
    "\n",
    "        # if we get here, we found a non-terminal token\n",
    "        # so we need to choose a replacement at random\n",
    "        replacement = sys_random.choice(grammar[token])\n",
    "        \n",
    "        if replacement == '_NN':\n",
    "            weight = [x/sum(dict_pwz_by_tag['_NN']) for x in dict_pwz_by_tag['_NN']]\n",
    "            replacement = nr.choice(grammar['_NN'], p=weight)\n",
    "#             print(grammar['_NN'], weight)\n",
    "            \n",
    "        if is_terminal(replacement):\n",
    "            tokens[i] = replacement\n",
    "        else:\n",
    "            tokens = tokens[:i] + replacement.split() + tokens[(i+1):]\n",
    "       \n",
    "        # now call expand on the new list of tokens\n",
    "        return expand(grammar, tokens, dict_pwz_by_tag)\n",
    "\n",
    "    # if we get here we had all terminals and are done\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(grammar, dict_pwz_by_tag):\n",
    "    return expand(grammar, [\"_S\"], dict_pwz_by_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentences_from_data(dict_data):\n",
    "    result = {}\n",
    "    for topic, words in dict_data.items():\n",
    "        sentence = []\n",
    "        grammar, dict_pwz_by_tag = create_grammar(words)\n",
    "#         print(grammar)\n",
    "        for s in range(10):\n",
    "            sentence.append(' '.join(generate_sentence(grammar, dict_pwz_by_tag)))\n",
    "        result = merge_two_dicts(result, {topic: sentence})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_story = create_sentences_from_data(dict(dict_ldapwz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ['pintu berbarengan bawah',\n",
       "  'bawah telah memotret aji',\n",
       "  'fox lagi pengin kamera',\n",
       "  'fox memotret pintu itu',\n",
       "  'pintu sampai bawah yang lemas',\n",
       "  'aji yang ingin kamera',\n",
       "  'bawah lemas',\n",
       "  'akses suka',\n",
       "  'pintu berbarengan',\n",
       "  'fox sampai weekend'],\n",
       " 1: ['paman pengin ruangan yang biasa',\n",
       "  'foto ingin basement bawah',\n",
       "  'lantai bekas',\n",
       "  'basement lelah',\n",
       "  'lantai bisa mencoba foto',\n",
       "  'foto yang ditutup lantai',\n",
       "  'lantai bisa ditutup lantai',\n",
       "  'basement adalah bawah',\n",
       "  'tanah ditutup jaman lantai',\n",
       "  'basement yang adalah lantai']}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09090909090909091, 0.8181818181818181, 0.09090909090909091]\n"
     ]
    }
   ],
   "source": [
    "elements = ['one', 'two', 'three'] \n",
    "weights = [0.001, 0.009, 0.001]\n",
    "new_weight = [x/sum(weights) for x in weights]\n",
    "\n",
    "from numpy.random import choice\n",
    "# print(choice(elements, p=new_weight))\n",
    "print(new_weight)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
